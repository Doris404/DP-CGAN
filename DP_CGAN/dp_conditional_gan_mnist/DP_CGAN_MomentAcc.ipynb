{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Our_Clean_DP_CGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ARBMZJjq9cw6",
        "XquJJMtx8WCc",
        "5Sp8pnnY-8gS",
        "-Er03CPx-w-X",
        "TLPCurGb_cx1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FMEADc0KDdAx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run all the following cells to get the output"
      ]
    },
    {
      "metadata": {
        "id": "0qLqkWmG6tfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56e6f865-d76c-4be5-d436-30abf262315e"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CNTlo6ri8Rj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Differential Privacy Classes\n",
        "\n",
        "Ref : Tensorflow Differentially Private package in Github"
      ]
    },
    {
      "metadata": {
        "id": "ARBMZJjq9cw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## dp_sgd.dp_optimizer.utils.py"
      ]
    },
    {
      "metadata": {
        "id": "y7kbj2Au9bB6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Utils for building and training NN models.\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class LayerParameters(object):\n",
        "  \"\"\"class that defines a non-conv layer.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.name = \"\"\n",
        "    self.num_units = 0\n",
        "    self._with_bias = False\n",
        "    self.relu = False\n",
        "    self.gradient_l2norm_bound = 0.0\n",
        "    self.bias_gradient_l2norm_bound = 0.0\n",
        "    self.trainable = True\n",
        "    self.weight_decay = 0.0\n",
        "\n",
        "\n",
        "class ConvParameters(object):\n",
        "  \"\"\"class that defines a conv layer.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.patch_size = 5\n",
        "    self.stride = 1\n",
        "    self.in_channels = 1\n",
        "    self.out_channels = 0\n",
        "    self.with_bias = True\n",
        "    self.relu = True\n",
        "    self.max_pool = True\n",
        "    self.max_pool_size = 2\n",
        "    self.max_pool_stride = 2\n",
        "    self.trainable = False\n",
        "    self.in_size = 28\n",
        "    self.name = \"\"\n",
        "    self.num_outputs = 0\n",
        "    self.bias_stddev = 0.1\n",
        "\n",
        "\n",
        "# Parameters for a layered neural network.\n",
        "class NetworkParameters(object):\n",
        "  \"\"\"class that define the overall model structure.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.input_size = 0\n",
        "    self.projection_type = 'NONE'  # NONE, RANDOM, PCA\n",
        "    self.projection_dimensions = 0\n",
        "    self.default_gradient_l2norm_bound = 0.0\n",
        "    self.layer_parameters = []  # List of LayerParameters\n",
        "    self.conv_parameters = []  # List of ConvParameters\n",
        "\n",
        "\n",
        "def GetTensorOpName(x):\n",
        "  \"\"\"Get the name of the op that created a tensor.\n",
        "\n",
        "  Useful for naming related tensors, as ':' in name field of op is not permitted\n",
        "\n",
        "  Args:\n",
        "    x: the input tensor.\n",
        "  Returns:\n",
        "    the name of the op.\n",
        "  \"\"\"\n",
        "\n",
        "  t = x.name.rsplit(\":\", 1)\n",
        "  if len(t) == 1:\n",
        "    return x.name\n",
        "  else:\n",
        "    return t[0]\n",
        "\n",
        "\n",
        "def BuildNetwork(inputs, network_parameters):\n",
        "  \"\"\"Build a network using the given parameters.\n",
        "\n",
        "  Args:\n",
        "    inputs: a Tensor of floats containing the input data.\n",
        "    network_parameters: NetworkParameters object\n",
        "      that describes the parameters for the network.\n",
        "  Returns:\n",
        "    output, training_parameters: where the outputs (a tensor) is the output\n",
        "      of the network, and training_parameters (a dictionary that maps the\n",
        "      name of each variable to a dictionary of parameters) is the parameters\n",
        "      used during training.\n",
        "  \"\"\"\n",
        "\n",
        "  training_parameters = {}\n",
        "  num_inputs = network_parameters.input_size\n",
        "  outputs = inputs\n",
        "  projection = None\n",
        "\n",
        "  # First apply convolutions, if needed\n",
        "  for conv_param in network_parameters.conv_parameters:\n",
        "    outputs = tf.reshape(\n",
        "        outputs,\n",
        "        [-1, conv_param.in_size, conv_param.in_size,\n",
        "         conv_param.in_channels])\n",
        "    conv_weights_name = \"%s_conv_weight\" % (conv_param.name)\n",
        "    conv_bias_name = \"%s_conv_bias\" % (conv_param.name)\n",
        "    conv_std_dev = 1.0 / (conv_param.patch_size\n",
        "                          * math.sqrt(conv_param.in_channels))\n",
        "    conv_weights = tf.Variable(\n",
        "        tf.truncated_normal([conv_param.patch_size,\n",
        "                             conv_param.patch_size,\n",
        "                             conv_param.in_channels,\n",
        "                             conv_param.out_channels],\n",
        "                            stddev=conv_std_dev),\n",
        "        trainable=conv_param.trainable,\n",
        "        name=conv_weights_name)\n",
        "    conv_bias = tf.Variable(\n",
        "        tf.truncated_normal([conv_param.out_channels],\n",
        "                            stddev=conv_param.bias_stddev),\n",
        "        trainable=conv_param.trainable,\n",
        "        name=conv_bias_name)\n",
        "    training_parameters[conv_weights_name] = {}\n",
        "    training_parameters[conv_bias_name] = {}\n",
        "    conv = tf.nn.conv2d(outputs, conv_weights,\n",
        "                        strides=[1, conv_param.stride,\n",
        "                                 conv_param.stride, 1],\n",
        "                        padding=\"SAME\")\n",
        "    relud = tf.nn.relu(conv + conv_bias)\n",
        "    mpd = tf.nn.max_pool(relud, ksize=[1,\n",
        "                                       conv_param.max_pool_size,\n",
        "                                       conv_param.max_pool_size, 1],\n",
        "                         strides=[1, conv_param.max_pool_stride,\n",
        "                                  conv_param.max_pool_stride, 1],\n",
        "                         padding=\"SAME\")\n",
        "    outputs = mpd\n",
        "    num_inputs = conv_param.num_outputs\n",
        "    # this should equal\n",
        "    # in_size * in_size * out_channels / (stride * max_pool_stride)\n",
        "\n",
        "  # once all the convs are done, reshape to make it flat\n",
        "  outputs = tf.reshape(outputs, [-1, num_inputs])\n",
        "\n",
        "  # Now project, if needed\n",
        "  if network_parameters.projection_type is not \"NONE\":\n",
        "    projection = tf.Variable(tf.truncated_normal(\n",
        "        [num_inputs, network_parameters.projection_dimensions],\n",
        "        stddev=1.0 / math.sqrt(num_inputs)), trainable=False, name=\"projection\")\n",
        "    num_inputs = network_parameters.projection_dimensions\n",
        "    outputs = tf.matmul(outputs, projection)\n",
        "\n",
        "  # Now apply any other layers\n",
        "\n",
        "  for layer_parameters in network_parameters.layer_parameters:\n",
        "    num_units = layer_parameters.num_units\n",
        "    hidden_weights_name = \"%s_weight\" % (layer_parameters.name)\n",
        "    hidden_weights = tf.Variable(\n",
        "        tf.truncated_normal([num_inputs, num_units],\n",
        "                            stddev=1.0 / math.sqrt(num_inputs)),\n",
        "        name=hidden_weights_name, trainable=layer_parameters.trainable)\n",
        "    training_parameters[hidden_weights_name] = {}\n",
        "    if layer_parameters.gradient_l2norm_bound:\n",
        "      training_parameters[hidden_weights_name][\"gradient_l2norm_bound\"] = (\n",
        "          layer_parameters.gradient_l2norm_bound)\n",
        "    if layer_parameters.weight_decay:\n",
        "      training_parameters[hidden_weights_name][\"weight_decay\"] = (\n",
        "          layer_parameters.weight_decay)\n",
        "\n",
        "    outputs = tf.matmul(outputs, hidden_weights)\n",
        "    if layer_parameters.with_bias:\n",
        "      hidden_biases_name = \"%s_bias\" % (layer_parameters.name)\n",
        "      hidden_biases = tf.Variable(tf.zeros([num_units]),\n",
        "                                  name=hidden_biases_name)\n",
        "      training_parameters[hidden_biases_name] = {}\n",
        "      if layer_parameters.bias_gradient_l2norm_bound:\n",
        "        training_parameters[hidden_biases_name][\n",
        "            \"bias_gradient_l2norm_bound\"] = (\n",
        "                layer_parameters.bias_gradient_l2norm_bound)\n",
        "\n",
        "      outputs += hidden_biases\n",
        "    if layer_parameters.relu:\n",
        "      outputs = tf.nn.relu(outputs)\n",
        "    # num_inputs for the next layer is num_units in the current layer.\n",
        "    num_inputs = num_units\n",
        "\n",
        "  return outputs, projection, training_parameters\n",
        "\n",
        "\n",
        "def VaryRate(start, end, saturate_epochs, epoch):\n",
        "  \"\"\"Compute a linearly varying number.\n",
        "\n",
        "  Decrease linearly from start to end until epoch saturate_epochs.\n",
        "\n",
        "  Args:\n",
        "    start: the initial number.\n",
        "    end: the end number.\n",
        "    saturate_epochs: after this we do not reduce the number; if less than\n",
        "      or equal to zero, just return start.\n",
        "    epoch: the current learning epoch.\n",
        "  Returns:\n",
        "    the caculated number.\n",
        "  \"\"\"\n",
        "  if saturate_epochs <= 0:\n",
        "    return start\n",
        "\n",
        "  step = (start - end) / (saturate_epochs - 1)\n",
        "  if epoch < saturate_epochs:\n",
        "    return start - step * epoch\n",
        "  else:\n",
        "    return end\n",
        "\n",
        "\n",
        "def BatchClipByL2norm(t, upper_bound, name=None):\n",
        "  \"\"\"Clip an array of tensors by L2 norm.\n",
        "\n",
        "  Shrink each dimension-0 slice of tensor (for matrix it is each row) such\n",
        "  that the l2 norm is at most upper_bound. Here we clip each row as it\n",
        "  corresponds to each example in the batch.\n",
        "\n",
        "  Args:\n",
        "    t: the input tensor.\n",
        "    upper_bound: the upperbound of the L2 norm.\n",
        "    name: optional name.\n",
        "  Returns:\n",
        "    the clipped tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  assert upper_bound > 0\n",
        "  with tf.name_scope(values=[t, upper_bound], name=name,\n",
        "                     default_name=\"batch_clip_by_l2norm\") as name:\n",
        "    saved_shape = tf.shape(t)\n",
        "    batch_size = tf.slice(saved_shape, [0], [1])\n",
        "    t2 = tf.reshape(t, tf.concat(axis=0, values=[batch_size, [-1]]))\n",
        "    upper_bound_inv = tf.fill(tf.slice(saved_shape, [0], [1]),\n",
        "                              tf.constant(1.0/upper_bound))\n",
        "    # Add a small number to avoid divide by 0\n",
        "    l2norm_inv = tf.rsqrt(tf.reduce_sum(t2 * t2, [1]) + 0.000001)\n",
        "    scale = tf.minimum(l2norm_inv, upper_bound_inv) * upper_bound\n",
        "    clipped_t = tf.matmul(tf.diag(scale), t2)\n",
        "    clipped_t = tf.reshape(clipped_t, saved_shape, name=name)\n",
        "  return clipped_t\n",
        "\n",
        "\n",
        "def SoftThreshold(t, threshold_ratio, name=None):\n",
        "  \"\"\"Soft-threshold a tensor by the mean value.\n",
        "\n",
        "  Softthreshold each dimension-0 vector (for matrix it is each column) by\n",
        "  the mean of absolute value multiplied by the threshold_ratio factor. Here\n",
        "  we soft threshold each column as it corresponds to each unit in a layer.\n",
        "\n",
        "  Args:\n",
        "    t: the input tensor.\n",
        "    threshold_ratio: the threshold ratio.\n",
        "    name: the optional name for the returned tensor.\n",
        "  Returns:\n",
        "    the thresholded tensor, where each entry is soft-thresholded by\n",
        "    threshold_ratio times the mean of the aboslute value of each column.\n",
        "  \"\"\"\n",
        "\n",
        "  assert threshold_ratio >= 0\n",
        "  with tf.name_scope(values=[t, threshold_ratio], name=name,\n",
        "                     default_name=\"soft_thresholding\") as name:\n",
        "    saved_shape = tf.shape(t)\n",
        "    t2 = tf.reshape(t, tf.concat(axis=0, values=[tf.slice(saved_shape, [0], [1]), -1]))\n",
        "    t_abs = tf.abs(t2)\n",
        "    t_x = tf.sign(t2) * tf.nn.relu(t_abs -\n",
        "                                   (tf.reduce_mean(t_abs, [0],\n",
        "                                                   keep_dims=True) *\n",
        "                                    threshold_ratio))\n",
        "    return tf.reshape(t_x, saved_shape, name=name)\n",
        "\n",
        "\n",
        "def AddGaussianNoise(t, sigma, name=None):\n",
        "  \"\"\"Add i.i.d. Gaussian noise (0, sigma^2) to every entry of t.\n",
        "\n",
        "  Args:\n",
        "    t: the input tensor.\n",
        "    sigma: the stddev of the Gaussian noise.\n",
        "    name: optional name.\n",
        "  Returns:\n",
        "    the noisy tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.name_scope(values=[t, sigma], name=name,\n",
        "                     default_name=\"add_gaussian_noise\") as name:\n",
        "    noisy_t = t + tf.random_normal(tf.shape(t), stddev=sigma)\n",
        "  return noisy_t\n",
        "\n",
        "\n",
        "def GenerateBinomialTable(m):\n",
        "  \"\"\"Generate binomial table.\n",
        "\n",
        "  Args:\n",
        "    m: the size of the table.\n",
        "  Returns:\n",
        "    A two dimensional array T where T[i][j] = (i choose j),\n",
        "    for 0<= i, j <=m.\n",
        "  \"\"\"\n",
        "\n",
        "  table = numpy.zeros((m + 1, m + 1), dtype=numpy.float64)\n",
        "  for i in range(m + 1):\n",
        "    table[i, 0] = 1\n",
        "  for i in range(1, m + 1):\n",
        "    for j in range(1, m + 1):\n",
        "      v = table[i - 1, j] + table[i - 1, j -1]\n",
        "      assert not math.isnan(v) and not math.isinf(v)\n",
        "      table[i, j] = v\n",
        "  return tf.convert_to_tensor(table)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XquJJMtx8WCc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## dp_sgd.dp_optimizer.sanitizer.py"
      ]
    },
    {
      "metadata": {
        "id": "4DyiidBz6_cz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Defines Sanitizer class for sanitizing tensors.\n",
        "\n",
        "A sanitizer first limits the sensitivity of a tensor and then adds noise\n",
        "to the tensor. The parameters are determined by the privacy_spending and the\n",
        "other parameters. It also uses an accountant to keep track of the privacy\n",
        "spending.\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "\n",
        "import collections\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#from differential_privacy.dp_sgd.dp_optimizer import utils\n",
        "\n",
        "\n",
        "ClipOption = collections.namedtuple(\"ClipOption\",\n",
        "                                    [\"l2norm_bound\", \"clip\"])\n",
        "\n",
        "\n",
        "class AmortizedGaussianSanitizer(object):\n",
        "  \"\"\"Sanitizer with Gaussian noise and amoritzed privacy spending accounting.\n",
        "\n",
        "  This sanitizes a tensor by first clipping the tensor, summing the tensor\n",
        "  and then adding appropriate amount of noise. It also uses an amortized\n",
        "  accountant to keep track of privacy spending.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, accountant, default_option):\n",
        "    \"\"\"Construct an AmortizedGaussianSanitizer.\n",
        "\n",
        "    Args:\n",
        "      accountant: the privacy accountant. Expect an amortized one.\n",
        "      default_option: the default ClipOptoin.\n",
        "    \"\"\"\n",
        "\n",
        "    self._accountant = accountant\n",
        "    self._default_option = default_option\n",
        "    self._options = {}\n",
        "\n",
        "  def set_option(self, tensor_name, option):\n",
        "    \"\"\"Set options for an individual tensor.\n",
        "\n",
        "    Args:\n",
        "      tensor_name: the name of the tensor.\n",
        "      option: clip option.\n",
        "    \"\"\"\n",
        "\n",
        "    self._options[tensor_name] = option\n",
        "\n",
        "  def sanitize(self, x, eps_delta, sigma=None,\n",
        "               option=ClipOption(None, None), tensor_name=None,\n",
        "               num_examples=None, add_noise=True):\n",
        "    \"\"\"Sanitize the given tensor.\n",
        "\n",
        "    This santize a given tensor by first applying l2 norm clipping and then\n",
        "    adding Gaussian noise. It calls the privacy accountant for updating the\n",
        "    privacy spending.\n",
        "\n",
        "    Args:\n",
        "      x: the tensor to sanitize.\n",
        "      eps_delta: a pair of eps, delta for (eps,delta)-DP. Use it to\n",
        "        compute sigma if sigma is None.\n",
        "      sigma: if sigma is not None, use sigma.\n",
        "      option: a ClipOption which, if supplied, used for\n",
        "        clipping and adding noise.\n",
        "      tensor_name: the name of the tensor.\n",
        "      num_examples: if None, use the number of \"rows\" of x.\n",
        "      add_noise: if True, then add noise, else just clip.\n",
        "    Returns:\n",
        "      a pair of sanitized tensor and the operation to accumulate privacy\n",
        "      spending.\n",
        "    \"\"\"\n",
        "\n",
        "    if sigma is None:\n",
        "      # pylint: disable=unpacking-non-sequence\n",
        "      eps, delta = eps_delta\n",
        "      with tf.control_dependencies(\n",
        "          [tf.Assert(tf.greater(eps, 0),\n",
        "                     [\"eps needs to be greater than 0\"]),\n",
        "           tf.Assert(tf.greater(delta, 0),\n",
        "                     [\"delta needs to be greater than 0\"])]):\n",
        "        # The following formula is taken from\n",
        "        #   Dwork and Roth, The Algorithmic Foundations of Differential\n",
        "        #   Privacy, Appendix A.\n",
        "        #   http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf\n",
        "        sigma = tf.sqrt(2.0 * tf.log(1.25 / delta)) / eps\n",
        "\n",
        "    l2norm_bound, clip = option\n",
        "    if l2norm_bound is None:\n",
        "      l2norm_bound, clip = self._default_option\n",
        "      if ((tensor_name is not None) and\n",
        "          (tensor_name in self._options)):\n",
        "        l2norm_bound, clip = self._options[tensor_name]\n",
        "    if clip:\n",
        "      x = BatchClipByL2norm(x, l2norm_bound)\n",
        "\n",
        "    if add_noise:\n",
        "      if num_examples is None:\n",
        "        num_examples = tf.slice(tf.shape(x), [0], [1])\n",
        "      privacy_accum_op = self._accountant.accumulate_privacy_spending(\n",
        "          eps_delta, sigma, num_examples)\n",
        "      with tf.control_dependencies([privacy_accum_op]):\n",
        "        saned_x = AddGaussianNoise(tf.reduce_sum(x, 0),\n",
        "                                         sigma * l2norm_bound)\n",
        "    else:\n",
        "      saned_x = tf.reduce_sum(x, 0)\n",
        "    return saned_x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Sp8pnnY-8gS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## differential_privacy.dp_sgd.per_example_gradients.per_example_gradients.py"
      ]
    },
    {
      "metadata": {
        "id": "q8py0NhV_C08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Per-example gradients for selected ops.\"\"\"\n",
        "\n",
        "import collections\n",
        "\n",
        "from six.moves import xrange\n",
        "import tensorflow as tf\n",
        "\n",
        "OrderedDict = collections.OrderedDict\n",
        "\n",
        "\n",
        "def _ListUnion(list_1, list_2):\n",
        "  \"\"\"Returns the union of two lists.\n",
        "\n",
        "  Python sets can have a non-deterministic iteration order. In some\n",
        "  contexts, this could lead to TensorFlow producing two different\n",
        "  programs when the same Python script is run twice. In these contexts\n",
        "  we use lists instead of sets.\n",
        "\n",
        "  This function is not designed to be especially fast and should only\n",
        "  be used with small lists.\n",
        "\n",
        "  Args:\n",
        "    list_1: A list\n",
        "    list_2: Another list\n",
        "\n",
        "  Returns:\n",
        "    A new list containing one copy of each unique element of list_1 and\n",
        "    list_2. Uniqueness is determined by \"x in union\" logic; e.g. two\n",
        "    string of that value appearing in the union.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: The arguments are not lists.\n",
        "  \"\"\"\n",
        "\n",
        "  if not (isinstance(list_1, list) and isinstance(list_2, list)):\n",
        "    raise TypeError(\"Arguments must be lists.\")\n",
        "\n",
        "  union = []\n",
        "  for x in list_1 + list_2:\n",
        "    if x not in union:\n",
        "      union.append(x)\n",
        "\n",
        "  return union\n",
        "\n",
        "\n",
        "def Interface(ys, xs):\n",
        "  \"\"\"Maps xs to consumers.\n",
        "\n",
        "    Returns a dict mapping each element of xs to any of its consumers that are\n",
        "    indirectly consumed by ys.\n",
        "\n",
        "  Args:\n",
        "    ys: The outputs\n",
        "    xs: The inputs\n",
        "  Returns:\n",
        "    out: Dict mapping each member x of `xs` to a list of all Tensors that are\n",
        "         direct consumers of x and are eventually consumed by a member of\n",
        "         `ys`.\n",
        "  \"\"\"\n",
        "\n",
        "  if isinstance(ys, (list, tuple)):\n",
        "    queue = list(ys)\n",
        "  else:\n",
        "    queue = [ys]\n",
        "\n",
        "  out = OrderedDict()\n",
        "  if isinstance(xs, (list, tuple)):\n",
        "    for x in xs:\n",
        "      out[x] = []\n",
        "  else:\n",
        "    out[xs] = []\n",
        "\n",
        "  done = set()\n",
        "\n",
        "  while queue:\n",
        "    y = queue.pop()\n",
        "    if y in done:\n",
        "      continue\n",
        "    done = done.union(set([y]))\n",
        "    for x in y.op.inputs:\n",
        "      if x in out:\n",
        "        out[x].append(y)\n",
        "      else:\n",
        "        assert id(x) not in [id(foo) for foo in out]\n",
        "    queue.extend(y.op.inputs)\n",
        "\n",
        "  return out\n",
        "\n",
        "\n",
        "class PXGRegistry(object):\n",
        "  \"\"\"Per-Example Gradient registry.\n",
        "\n",
        "  Maps names of ops to per-example gradient rules for those ops.\n",
        "  These rules are only needed for ops that directly touch values that\n",
        "  are shared between examples. For most machine learning applications,\n",
        "  this means only ops that directly operate on the parameters.\n",
        "\n",
        "\n",
        "  See http://arxiv.org/abs/1510.01799 for more information, and please\n",
        "  consider citing that tech report if you use this function in published\n",
        "  research.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.d = OrderedDict()\n",
        "\n",
        "  def __call__(self, op,\n",
        "               colocate_gradients_with_ops=False,\n",
        "               gate_gradients=False):\n",
        "    if op.node_def.op not in self.d:\n",
        "      raise NotImplementedError(\"No per-example gradient rule registered \"\n",
        "                                \"for \" + op.node_def.op + \" in pxg_registry.\")\n",
        "    return self.d[op.node_def.op](op,\n",
        "                                  colocate_gradients_with_ops,\n",
        "                                  gate_gradients)\n",
        "\n",
        "  def Register(self, op_name, pxg_class):\n",
        "    \"\"\"Associates `op_name` key with `pxg_class` value.\n",
        "\n",
        "    Registers `pxg_class` as the class that will be called to perform\n",
        "    per-example differentiation through ops with `op_name`.\n",
        "\n",
        "    Args:\n",
        "      op_name: String op name.\n",
        "      pxg_class: An instance of any class with the same signature as MatMulPXG.\n",
        "    \"\"\"\n",
        "    self.d[op_name] = pxg_class\n",
        "\n",
        "\n",
        "pxg_registry = PXGRegistry()\n",
        "\n",
        "\n",
        "class MatMulPXG(object):\n",
        "  \"\"\"Per-example gradient rule for MatMul op.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, op,\n",
        "               colocate_gradients_with_ops=False,\n",
        "               gate_gradients=False):\n",
        "    \"\"\"Construct an instance of the rule for `op`.\n",
        "\n",
        "    Args:\n",
        "      op: The Operation to differentiate through.\n",
        "      colocate_gradients_with_ops: currently unsupported\n",
        "      gate_gradients: currently unsupported\n",
        "    \"\"\"\n",
        "    assert op.node_def.op == \"MatMul\"\n",
        "    self.op = op\n",
        "    self.colocate_gradients_with_ops = colocate_gradients_with_ops\n",
        "    self.gate_gradients = gate_gradients\n",
        "\n",
        "  def __call__(self, x, z_grads):\n",
        "    \"\"\"Build the graph for the per-example gradient through the op.\n",
        "\n",
        "    Assumes that the MatMul was called with a design matrix with examples\n",
        "    in rows as the first argument and parameters as the second argument.\n",
        "\n",
        "    Args:\n",
        "      x: The Tensor to differentiate with respect to. This tensor must\n",
        "         represent the weights.\n",
        "      z_grads: The list of gradients on the output of the op.\n",
        "\n",
        "    Returns:\n",
        "      x_grads: A Tensor containing the gradient with respect to `x` for\n",
        "       each example. This is a 3-D tensor, with the first axis corresponding\n",
        "       to examples and the remaining axes matching the shape of x.\n",
        "    \"\"\"\n",
        "    idx = list(self.op.inputs).index(x)\n",
        "    assert idx != -1\n",
        "    assert len(z_grads) == len(self.op.outputs)\n",
        "    assert idx == 1  # We expect weights to be arg 1\n",
        "    # We don't expect anyone to per-example differentiate with repsect\n",
        "    # to anything other than the weights.\n",
        "    x, _ = self.op.inputs\n",
        "    z_grads, = z_grads\n",
        "    x_expanded = tf.expand_dims(x, 2)\n",
        "    z_grads_expanded = tf.expand_dims(z_grads, 1)\n",
        "    return tf.multiply(x_expanded, z_grads_expanded)\n",
        "\n",
        "\n",
        "pxg_registry.Register(\"MatMul\", MatMulPXG)\n",
        "\n",
        "\n",
        "class Conv2DPXG(object):\n",
        "  \"\"\"Per-example gradient rule of Conv2d op.\n",
        "\n",
        "  Same interface as MatMulPXG.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, op,\n",
        "               colocate_gradients_with_ops=False,\n",
        "               gate_gradients=False):\n",
        "\n",
        "    assert op.node_def.op == \"Conv2D\"\n",
        "    self.op = op\n",
        "    self.colocate_gradients_with_ops = colocate_gradients_with_ops\n",
        "    self.gate_gradients = gate_gradients\n",
        "\n",
        "  def _PxConv2DBuilder(self, input_, w, strides, padding):\n",
        "    \"\"\"conv2d run separately per example, to help compute per-example gradients.\n",
        "\n",
        "    Args:\n",
        "      input_: tensor containing a minibatch of images / feature maps.\n",
        "              Shape [batch_size, rows, columns, channels]\n",
        "      w: convolution kernels. Shape\n",
        "        [kernel rows, kernel columns, input channels, output channels]\n",
        "      strides: passed through to regular conv_2d\n",
        "      padding: passed through to regular conv_2d\n",
        "\n",
        "    Returns:\n",
        "      conv: the output of the convolution.\n",
        "         single tensor, same as what regular conv_2d does\n",
        "      w_px: a list of batch_size copies of w. each copy was used\n",
        "          for the corresponding example in the minibatch.\n",
        "           calling tf.gradients on the copy gives the gradient for just\n",
        "                  that example.\n",
        "    \"\"\"\n",
        "    input_shape = [int(e) for e in input_.get_shape()]\n",
        "    batch_size = input_shape[0]\n",
        "    input_px = [tf.slice(\n",
        "        input_, [example] + [0] * 3, [1] + input_shape[1:]) for example\n",
        "                in xrange(batch_size)]\n",
        "    for input_x in input_px:\n",
        "      assert int(input_x.get_shape()[0]) == 1\n",
        "    w_px = [tf.identity(w) for example in xrange(batch_size)]\n",
        "    conv_px = [tf.nn.conv2d(input_x, w_x,\n",
        "                            strides=strides,\n",
        "                            padding=padding)\n",
        "               for input_x, w_x in zip(input_px, w_px)]\n",
        "    for conv_x in conv_px:\n",
        "      num_x = int(conv_x.get_shape()[0])\n",
        "      assert num_x == 1, num_x\n",
        "    assert len(conv_px) == batch_size\n",
        "    conv = tf.concat(axis=0, values=conv_px)\n",
        "    assert int(conv.get_shape()[0]) == batch_size\n",
        "    return conv, w_px\n",
        "\n",
        "  def __call__(self, w, z_grads):\n",
        "    idx = list(self.op.inputs).index(w)\n",
        "    # Make sure that `op` was actually applied to `w`\n",
        "    assert idx != -1\n",
        "    assert len(z_grads) == len(self.op.outputs)\n",
        "    # The following assert may be removed when we are ready to use this\n",
        "    # for general purpose code.\n",
        "    # This assert is only expected to hold in the contex of our preliminary\n",
        "    # MNIST experiments.\n",
        "    assert idx == 1  # We expect convolution weights to be arg 1\n",
        "\n",
        "    images, filters = self.op.inputs\n",
        "    strides = self.op.get_attr(\"strides\")\n",
        "    padding = self.op.get_attr(\"padding\")\n",
        "    # Currently assuming that one specifies at most these four arguments and\n",
        "    # that all other arguments to conv2d are set to default.\n",
        "\n",
        "    conv, w_px = self._PxConv2DBuilder(images, filters, strides, padding)\n",
        "    z_grads, = z_grads\n",
        "\n",
        "    gradients_list = tf.gradients(conv, w_px, z_grads,\n",
        "                                  colocate_gradients_with_ops=\n",
        "                                  self.colocate_gradients_with_ops,\n",
        "                                  gate_gradients=self.gate_gradients)\n",
        "\n",
        "    return tf.stack(gradients_list)\n",
        "\n",
        "pxg_registry.Register(\"Conv2D\", Conv2DPXG)\n",
        "\n",
        "\n",
        "class AddPXG(object):\n",
        "  \"\"\"Per-example gradient rule for Add op.\n",
        "\n",
        "  Same interface as MatMulPXG.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, op,\n",
        "               colocate_gradients_with_ops=False,\n",
        "               gate_gradients=False):\n",
        "\n",
        "    assert op.node_def.op == \"Add\"\n",
        "    self.op = op\n",
        "    self.colocate_gradients_with_ops = colocate_gradients_with_ops\n",
        "    self.gate_gradients = gate_gradients\n",
        "\n",
        "  def __call__(self, x, z_grads):\n",
        "    idx = list(self.op.inputs).index(x)\n",
        "    # Make sure that `op` was actually applied to `x`\n",
        "    assert idx != -1\n",
        "    assert len(z_grads) == len(self.op.outputs)\n",
        "    # The following assert may be removed when we are ready to use this\n",
        "    # for general purpose code.\n",
        "    # This assert is only expected to hold in the contex of our preliminary\n",
        "    # MNIST experiments.\n",
        "    assert idx == 1 # We expect biases to be arg 1\n",
        "    # We don't expect anyone to per-example differentiate with respect\n",
        "    # to anything other than the biases.\n",
        "    x, _ = self.op.inputs\n",
        "    z_grads, = z_grads\n",
        "    return z_grads\n",
        "\n",
        "\n",
        "pxg_registry.Register(\"Add\", AddPXG)\n",
        "\n",
        "\n",
        "def PerExampleGradients(ys, xs, grad_ys=None, name=\"gradients\",\n",
        "                        colocate_gradients_with_ops=False,\n",
        "                        gate_gradients=False):\n",
        "  \"\"\"Symbolic differentiation, separately for each example.\n",
        "\n",
        "  Matches the interface of tf.gradients, but the return values each have an\n",
        "  additional axis corresponding to the examples.\n",
        "\n",
        "  Assumes that the cost in `ys` is additive across examples.\n",
        "  e.g., no batch normalization.\n",
        "  Individual rules for each op specify their own assumptions about how\n",
        "  examples are put into tensors.\n",
        "  \"\"\"\n",
        "\n",
        "  # Find the interface between the xs and the cost\n",
        "  for x in xs:\n",
        "    assert isinstance(x, tf.Tensor), type(x)\n",
        "  interface = Interface(ys, xs)\n",
        "  merged_interface = []\n",
        "  for x in xs:\n",
        "    merged_interface = _ListUnion(merged_interface, interface[x])\n",
        "  # Differentiate with respect to the interface\n",
        "  interface_gradients = tf.gradients(ys, merged_interface, grad_ys=grad_ys,\n",
        "                                     name=name,\n",
        "                                     colocate_gradients_with_ops=\n",
        "                                     colocate_gradients_with_ops,\n",
        "                                     gate_gradients=gate_gradients)\n",
        "  grad_dict = OrderedDict(zip(merged_interface, interface_gradients))\n",
        "  # Build the per-example gradients with respect to the xs\n",
        "  if colocate_gradients_with_ops:\n",
        "    raise NotImplementedError(\"The per-example gradients are not yet \"\n",
        "                              \"colocated with ops.\")\n",
        "  if gate_gradients:\n",
        "    raise NotImplementedError(\"The per-example gradients are not yet \"\n",
        "                              \"gated.\")\n",
        "  out = []\n",
        "  for x in xs:\n",
        "    zs = interface[x]\n",
        "    ops = []\n",
        "    for z in zs:\n",
        "      ops = _ListUnion(ops, [z.op])\n",
        "    if len(ops) != 1:\n",
        "      raise NotImplementedError(\"Currently we only support the case \"\n",
        "                                \"where each x is consumed by exactly \"\n",
        "                                \"one op. but %s is consumed by %d ops.\"\n",
        "                                % (x.name, len(ops)))\n",
        "    op = ops[0]\n",
        "    pxg_rule = pxg_registry(op, colocate_gradients_with_ops, gate_gradients)\n",
        "    x_grad = pxg_rule(x, [grad_dict[z] for z in zs])\n",
        "    out.append(x_grad)\n",
        "  return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Er03CPx-w-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## dp_sgd.dp_optimizer.dp_optimizer.py"
      ]
    },
    {
      "metadata": {
        "id": "lJYBti0P-ydy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Differentially private optimizers.\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#from differential_privacy.dp_sgd.dp_optimizer import utils\n",
        "#from differential_privacy.dp_sgd.per_example_gradients import per_example_gradients\n",
        "\n",
        "\n",
        "class DPGradientDescentOptimizer(tf.train.GradientDescentOptimizer):\n",
        "    \"\"\"Differentially private gradient descent optimizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate, eps_delta, sanitizer,\n",
        "                 sigma=None, use_locking=False, name=\"DPGradientDescent\",\n",
        "                 batches_per_lot=1):\n",
        "        \"\"\"Construct a differentially private gradient descent optimizer.\n",
        "\n",
        "        The optimizer uses fixed privacy budget for each batch of training.\n",
        "\n",
        "        Args:\n",
        "          learning_rate: for GradientDescentOptimizer.\n",
        "          eps_delta: EpsDelta pair for each epoch.\n",
        "          sanitizer: for sanitizing the graident.\n",
        "          sigma: noise sigma. If None, use eps_delta pair to compute sigma;\n",
        "            otherwise use supplied sigma directly.\n",
        "          use_locking: use locking.\n",
        "          name: name for the object.\n",
        "          batches_per_lot: Number of batches in a lot.\n",
        "        \"\"\"\n",
        "\n",
        "        super(DPGradientDescentOptimizer, self).__init__(learning_rate,\n",
        "                                                         use_locking, name)\n",
        "\n",
        "        # Also, if needed, define the gradient accumulators\n",
        "        self._batches_per_lot = batches_per_lot\n",
        "        self._grad_accum_dict = {}\n",
        "        if batches_per_lot > 1:\n",
        "            self._batch_count = tf.Variable(1, dtype=tf.int32, trainable=False,\n",
        "                                            name=\"batch_count\")\n",
        "            var_list = tf.trainable_variables()\n",
        "            with tf.variable_scope(\"grad_acc_for\"):\n",
        "                for var in var_list:\n",
        "                    v_grad_accum = tf.Variable(tf.zeros_like(var),\n",
        "                                               trainable=False,\n",
        "                                               name=GetTensorOpName(var))\n",
        "                    self._grad_accum_dict[var.name] = v_grad_accum\n",
        "\n",
        "        self._eps_delta = eps_delta\n",
        "        self._sanitizer = sanitizer\n",
        "        self._sigma = sigma\n",
        "\n",
        "    def compute_sanitized_gradients(self, loss, var_list=None,\n",
        "                                    add_noise=True):\n",
        "        \"\"\"Compute the sanitized gradients.\n",
        "\n",
        "        Args:\n",
        "          loss: the loss tensor.\n",
        "          var_list: the optional variables.\n",
        "          add_noise: if true, then add noise. Always clip.\n",
        "        Returns:\n",
        "          a pair of (list of sanitized gradients) and privacy spending accumulation\n",
        "          operations.\n",
        "        Raises:\n",
        "          TypeError: if var_list contains non-variable.\n",
        "        \"\"\"\n",
        "\n",
        "        self._assert_valid_dtypes([loss])\n",
        "\n",
        "        xs = [tf.convert_to_tensor(x) for x in var_list]\n",
        "        px_grads = PerExampleGradients(loss, xs)\n",
        "        sanitized_grads = []\n",
        "        for px_grad, v in zip(px_grads, var_list):\n",
        "            tensor_name = GetTensorOpName(v)\n",
        "            sanitized_grad = self._sanitizer.sanitize(\n",
        "                px_grad, self._eps_delta, sigma=self._sigma,\n",
        "                tensor_name=tensor_name, add_noise=add_noise,\n",
        "                num_examples=self._batches_per_lot * tf.slice(\n",
        "                    tf.shape(px_grad), [0], [1]))\n",
        "            sanitized_grads.append(sanitized_grad)\n",
        "\n",
        "        return sanitized_grads\n",
        "\n",
        "    def minimize(self, loss, global_step=None, var_list=None,\n",
        "                 name=None):\n",
        "        \"\"\"Minimize using sanitized gradients.\n",
        "\n",
        "        This gets a var_list which is the list of trainable variables.\n",
        "        For each var in var_list, we defined a grad_accumulator variable\n",
        "        during init. When batches_per_lot > 1, we accumulate the gradient\n",
        "        update in those. At the end of each lot, we apply the update back to\n",
        "        the variable. This has the effect that for each lot we compute\n",
        "        gradients at the point at the beginning of the lot, and then apply one\n",
        "        update at the end of the lot. In other words, semantically, we are doing\n",
        "        SGD with one lot being the equivalent of one usual batch of size\n",
        "        batch_size * batches_per_lot.\n",
        "        This allows us to simulate larger batches than our memory size would permit.\n",
        "\n",
        "        The lr and the num_steps are in the lot world.\n",
        "\n",
        "        Args:\n",
        "          loss: the loss tensor.\n",
        "          global_step: the optional global step.\n",
        "          var_list: the optional variables.\n",
        "          name: the optional name.\n",
        "        Returns:\n",
        "          the operation that runs one step of DP gradient descent.\n",
        "        \"\"\"\n",
        "\n",
        "        # First validate the var_list\n",
        "\n",
        "        if var_list is None:\n",
        "            var_list = tf.trainable_variables()\n",
        "        for var in var_list:\n",
        "            if not isinstance(var, tf.Variable):\n",
        "                raise TypeError(\"Argument is not a variable.Variable: %s\" % var)\n",
        "\n",
        "        # Modification: apply gradient once every batches_per_lot many steps.\n",
        "        # This may lead to smaller error\n",
        "\n",
        "        if self._batches_per_lot == 1:\n",
        "            sanitized_grads = self.compute_sanitized_gradients(\n",
        "                loss, var_list=var_list)\n",
        "\n",
        "            grads_and_vars = list(zip(sanitized_grads, var_list))\n",
        "            self._assert_valid_dtypes([v for g, v in grads_and_vars if g is not None])\n",
        "\n",
        "            apply_grads = self.apply_gradients(grads_and_vars,\n",
        "                                               global_step=global_step, name=name)\n",
        "            return apply_grads\n",
        "\n",
        "        # Condition for deciding whether to accumulate the gradient\n",
        "        # or actually apply it.\n",
        "        # we use a private self_batch_count to keep track of number of batches.\n",
        "        # global step will count number of lots processed.\n",
        "\n",
        "        update_cond = tf.equal(tf.constant(0),\n",
        "                               tf.mod(self._batch_count,\n",
        "                                      tf.constant(self._batches_per_lot)))\n",
        "\n",
        "        # Things to do for batches other than last of the lot.\n",
        "        # Add non-noisy clipped grads to shadow variables.\n",
        "        \n",
        "# ----------------------------------  minimize_ours: ----------------------------------------------------------------\n",
        "\n",
        "    def minimize_ours(self, d_loss_real, d_loss_fake, global_step=None, var_list=None,\n",
        "                      name=None):\n",
        "        \"\"\"Minimize using sanitized gradients\n",
        "\n",
        "        Args:\n",
        "          d_loss_real: the loss tensor for real data\n",
        "          d_loss_fake: the loss tensor for fake data\n",
        "          global_step: the optional global step.\n",
        "          var_list: the optional variables.\n",
        "          name: the optional name.\n",
        "        Returns:\n",
        "          the operation that runs one step of DP gradient descent.\n",
        "        \"\"\"\n",
        "\n",
        "        # First validate the var_list\n",
        "\n",
        "        if var_list is None:\n",
        "            var_list = tf.trainable_variables()\n",
        "        for var in var_list:\n",
        "            if not isinstance(var, tf.Variable):\n",
        "                raise TypeError(\"Argument is not a variable.Variable: %s\" % var)\n",
        "\n",
        "        # Modification: apply gradient once every batches_per_lot many steps.\n",
        "        # This may lead to smaller error\n",
        "\n",
        "        if self._batches_per_lot == 1:\n",
        "            \n",
        "            # ------------------  OUR METHOD --------------------------------\n",
        "            #compute_sanitized_gradients for fake data after clipping the gradients (without adding noise)\n",
        "            f_grads_and_vars = self.compute_sanitized_gradients(d_loss_fake, var_list=var_list, add_noise=False)\n",
        "            \n",
        "            #compute_sanitized_gradients for real data : clip the gradients and then add noise to them\n",
        "            r_grads_and_vars = self.compute_sanitized_gradients(d_loss_real, var_list=var_list, add_noise=True)\n",
        "            \n",
        "            # Compute the overall gradients by combining the computed gradients for real data and fake data\n",
        "            s_grads_and_vars = [(r_grads_and_vars[idx] + f_grads_and_vars[idx]) for idx in\n",
        "                                range(len(r_grads_and_vars))]\n",
        "            \n",
        "            sanitized_grads_and_vars = list(zip(s_grads_and_vars, var_list))\n",
        "            self._assert_valid_dtypes([v for g, v in sanitized_grads_and_vars if g is not None])\n",
        "            \n",
        "            # Apply the overall gradients\n",
        "            apply_grads = self.apply_gradients(sanitized_grads_and_vars, global_step=global_step, name=name)\n",
        "\n",
        "            return apply_grads\n",
        "# ---------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        update_cond = tf.equal(tf.constant(0),\n",
        "                               tf.mod(self._batch_count,\n",
        "                                      tf.constant(self._batches_per_lot)))\n",
        "\n",
        "    def non_last_in_lot_op(loss, var_list):\n",
        "        \"\"\"Ops to do for a typical batch.\n",
        "\n",
        "        For a batch that is not the last one in the lot, we simply compute the\n",
        "        sanitized gradients and apply them to the grad_acc variables.\n",
        "\n",
        "        Args:\n",
        "          loss: loss function tensor\n",
        "          var_list: list of variables\n",
        "        Returns:\n",
        "          A tensorflow op to do the updates to the gradient accumulators\n",
        "        \"\"\"\n",
        "        sanitized_grads = self.compute_sanitized_gradients(\n",
        "            loss, var_list=var_list, add_noise=False)\n",
        "\n",
        "        update_ops_list = []\n",
        "        for var, grad in zip(var_list, sanitized_grads):\n",
        "            grad_acc_v = self._grad_accum_dict[var.name]\n",
        "            update_ops_list.append(grad_acc_v.assign_add(grad))\n",
        "        update_ops_list.append(self._batch_count.assign_add(1))\n",
        "        return tf.group(*update_ops_list)\n",
        "\n",
        "        # Things to do for last batch of a lot.\n",
        "        # Add noisy clipped grads to accumulator.\n",
        "        # Apply accumulated grads to vars.\n",
        "\n",
        "    def last_in_lot_op(loss, var_list, global_step):\n",
        "        \"\"\"Ops to do for last batch in a lot.\n",
        "\n",
        "        For the last batch in the lot, we first add the sanitized gradients to\n",
        "        the gradient acc variables, and then apply these\n",
        "        values over to the original variables (via an apply gradient)\n",
        "\n",
        "        Args:\n",
        "          loss: loss function tensor\n",
        "          var_list: list of variables\n",
        "          global_step: optional global step to be passed to apply_gradients\n",
        "        Returns:\n",
        "          A tensorflow op to push updates from shadow vars to real vars.\n",
        "        \"\"\"\n",
        "\n",
        "        # We add noise in the last lot. This is why we need this code snippet\n",
        "        # that looks almost identical to the non_last_op case here.\n",
        "        sanitized_grads = self.compute_sanitized_gradients(\n",
        "            loss, var_list=var_list, add_noise=True)\n",
        "\n",
        "        normalized_grads = []\n",
        "        for var, grad in zip(var_list, sanitized_grads):\n",
        "            grad_acc_v = self._grad_accum_dict[var.name]\n",
        "            # To handle the lr difference per lot vs per batch, we divide the\n",
        "            # update by number of batches per lot.\n",
        "            normalized_grad = tf.div(grad_acc_v.assign_add(grad),\n",
        "                                     tf.to_float(self._batches_per_lot))\n",
        "\n",
        "            normalized_grads.append(normalized_grad)\n",
        "\n",
        "        with tf.control_dependencies(normalized_grads):\n",
        "            grads_and_vars = zip(normalized_grads, var_list)\n",
        "            self._assert_valid_dtypes(\n",
        "                [v for g, v in grads_and_vars if g is not None])\n",
        "            apply_san_grads = self.apply_gradients(grads_and_vars,\n",
        "                                                   global_step=global_step,\n",
        "                                                   name=\"apply_grads\")\n",
        "\n",
        "        # Now reset the accumulators to zero\n",
        "        resets_list = []\n",
        "        with tf.control_dependencies([apply_san_grads]):\n",
        "            for _, acc in self._grad_accum_dict.items():\n",
        "                reset = tf.assign(acc, tf.zeros_like(acc))\n",
        "                resets_list.append(reset)\n",
        "        resets_list.append(self._batch_count.assign_add(1))\n",
        "\n",
        "        last_step_update = tf.group(*([apply_san_grads] + resets_list))\n",
        "        return last_step_update\n",
        "        # pylint: disable=g-long-lambda\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLPCurGb_cx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## differential_privacy.privacy_accountant.tf.accountant.py"
      ]
    },
    {
      "metadata": {
        "id": "al98tQBl_tJ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Defines Accountant class for keeping track of privacy spending.\n",
        "\n",
        "A privacy accountant keeps track of privacy spendings. It has methods\n",
        "accumulate_privacy_spending and get_privacy_spent. Here we only define\n",
        "AmortizedAccountant which tracks the privacy spending in the amortized\n",
        "way. It uses privacy amplication via sampling to compute the privacy\n",
        "spending for each batch and strong composition (specialized for Gaussian\n",
        "noise) for accumulate the privacy spending.\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "\n",
        "import abc\n",
        "import collections\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "#from differential_privacy.dp_sgd.dp_optimizer import utils\n",
        "\n",
        "EpsDelta = collections.namedtuple(\"EpsDelta\", [\"spent_eps\", \"spent_delta\"])\n",
        "\n",
        "\n",
        "# TODO(liqzhang) To ensure the same API for AmortizedAccountant and\n",
        "# MomentsAccountant, we pass the union of arguments to both, so we\n",
        "# have unused_sigma for AmortizedAccountant and unused_eps_delta for\n",
        "# MomentsAccountant. Consider to revise the API to avoid the unused\n",
        "# arguments.  It would be good to use @abc.abstractmethod, etc, to\n",
        "# define the common interface as a base class.\n",
        "class AmortizedAccountant(object):\n",
        "  \"\"\"Keep track of privacy spending in an amortized way.\n",
        "\n",
        "  AmortizedAccountant accumulates the privacy spending by assuming\n",
        "  all the examples are processed uniformly at random so the spending is\n",
        "  amortized among all the examples. And we assume that we use Gaussian noise\n",
        "  so the accumulation is on eps^2 and delta, using advanced composition.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, total_examples):\n",
        "    \"\"\"Initialization. Currently only support amortized tracking.\n",
        "\n",
        "    Args:\n",
        "      total_examples: total number of examples.\n",
        "    \"\"\"\n",
        "\n",
        "    assert total_examples > 0\n",
        "    self._total_examples = total_examples\n",
        "    self._eps_squared_sum = tf.Variable(tf.zeros([1]), trainable=False,\n",
        "                                        name=\"eps_squared_sum\")\n",
        "    self._delta_sum = tf.Variable(tf.zeros([1]), trainable=False,\n",
        "                                  name=\"delta_sum\")\n",
        "\n",
        "  def accumulate_privacy_spending(self, eps_delta, unused_sigma,\n",
        "                                  num_examples):\n",
        "    \"\"\"Accumulate the privacy spending.\n",
        "\n",
        "    Currently only support approximate privacy. Here we assume we use Gaussian\n",
        "    noise on randomly sampled batch so we get better composition: 1. the per\n",
        "    batch privacy is computed using privacy amplication via sampling bound;\n",
        "    2. the composition is done using the composition with Gaussian noise.\n",
        "    TODO(liqzhang) Add a link to a document that describes the bounds used.\n",
        "\n",
        "    Args:\n",
        "      eps_delta: EpsDelta pair which can be tensors.\n",
        "      unused_sigma: the noise sigma. Unused for this accountant.\n",
        "      num_examples: the number of examples involved.\n",
        "    Returns:\n",
        "      a TensorFlow operation for updating the privacy spending.\n",
        "    \"\"\"\n",
        "\n",
        "    eps, delta = eps_delta\n",
        "    with tf.control_dependencies(\n",
        "        [tf.Assert(tf.greater(delta, 0),\n",
        "                   [\"delta needs to be greater than 0\"])]):\n",
        "      amortize_ratio = (tf.cast(num_examples, tf.float32) * 1.0 /\n",
        "                        self._total_examples)\n",
        "      # Use privacy amplification via sampling bound.\n",
        "      # See Lemma 2.2 in http://arxiv.org/pdf/1405.7085v2.pdf\n",
        "      # TODO(liqzhang) Add a link to a document with formal statement\n",
        "      # and proof.\n",
        "      amortize_eps = tf.reshape(tf.log(1.0 + amortize_ratio * (\n",
        "          tf.exp(eps) - 1.0)), [1])\n",
        "      amortize_delta = tf.reshape(amortize_ratio * delta, [1])\n",
        "      return tf.group(*[tf.assign_add(self._eps_squared_sum,\n",
        "                                      tf.square(amortize_eps)),\n",
        "                        tf.assign_add(self._delta_sum, amortize_delta)])\n",
        "\n",
        "  def get_privacy_spent(self, sess, target_eps=None):\n",
        "    \"\"\"Report the spending so far.\n",
        "\n",
        "    Args:\n",
        "      sess: the session to run the tensor.\n",
        "      target_eps: the target epsilon. Unused.\n",
        "    Returns:\n",
        "      the list containing a single EpsDelta, with values as Python floats (as\n",
        "      opposed to numpy.float64). This is to be consistent with\n",
        "      MomentAccountant which can return a list of (eps, delta) pair.\n",
        "    \"\"\"\n",
        "\n",
        "    # pylint: disable=unused-argument\n",
        "    unused_target_eps = target_eps\n",
        "    eps_squared_sum, delta_sum = sess.run([self._eps_squared_sum,\n",
        "                                           self._delta_sum])\n",
        "    return [EpsDelta(math.sqrt(eps_squared_sum), float(delta_sum))]\n",
        "\n",
        "\n",
        "class MomentsAccountant(object):\n",
        "  \"\"\"Privacy accountant which keeps track of moments of privacy loss.\n",
        "\n",
        "  Note: The constructor of this class creates tf.Variables that must\n",
        "  be initialized with tf.global_variables_initializer() or similar calls.\n",
        "\n",
        "  MomentsAccountant accumulates the high moments of the privacy loss. It\n",
        "  requires a method for computing differenital moments of the noise (See\n",
        "  below for the definition). So every specific accountant should subclass\n",
        "  this class by implementing _differential_moments method.\n",
        "\n",
        "  Denote by X_i the random variable of privacy loss at the i-th step.\n",
        "  Consider two databases D, D' which differ by one item. X_i takes value\n",
        "  log Pr[M(D')==x]/Pr[M(D)==x] with probability Pr[M(D)==x].\n",
        "  In MomentsAccountant, we keep track of y_i(L) = log E[exp(L X_i)] for some\n",
        "  large enough L. To compute the final privacy spending,  we apply Chernoff\n",
        "  bound (assuming the random noise added at each step is independent) to\n",
        "  bound the total privacy loss Z = sum X_i as follows:\n",
        "    Pr[Z > e] = Pr[exp(L Z) > exp(L e)]\n",
        "              < E[exp(L Z)] / exp(L e)\n",
        "              = Prod_i E[exp(L X_i)] / exp(L e)\n",
        "              = exp(sum_i log E[exp(L X_i)]) / exp(L e)\n",
        "              = exp(sum_i y_i(L) - L e)\n",
        "  Hence the mechanism is (e, d)-differentially private for\n",
        "    d =  exp(sum_i y_i(L) - L e).\n",
        "  We require d < 1, i.e. e > sum_i y_i(L) / L. We maintain y_i(L) for several\n",
        "  L to compute the best d for any give e (normally should be the lowest L\n",
        "  such that 2 * sum_i y_i(L) / L < e.\n",
        "\n",
        "  We further assume that at each step, the mechanism operates on a random\n",
        "  sample with sampling probability q = batch_size / total_examples. Then\n",
        "    E[exp(L X)] = E[(Pr[M(D)==x / Pr[M(D')==x])^L]\n",
        "  By distinguishing two cases of whether D < D' or D' < D, we have\n",
        "  that\n",
        "    E[exp(L X)] <= max (I1, I2)\n",
        "  where\n",
        "    I1 = (1-q) E ((1-q) + q P(X+1) / P(X))^L + q E ((1-q) + q P(X) / P(X-1))^L\n",
        "    I2 = E (P(X) / ((1-q) + q P(X+1)))^L\n",
        "\n",
        "  In order to compute I1 and I2, one can consider to\n",
        "    1. use an asymptotic bound, which recovers the advance composition theorem;\n",
        "    2. use the closed formula (like GaussianMomentsAccountant);\n",
        "    3. use numerical integration or random sample estimation.\n",
        "\n",
        "  Dependent on the distribution, we can often obtain a tigher estimation on\n",
        "  the moments and hence a more accurate estimation of the privacy loss than\n",
        "  obtained using generic composition theorems.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  __metaclass__ = abc.ABCMeta\n",
        "\n",
        "  def __init__(self, total_examples, moment_orders=32):\n",
        "    \"\"\"Initialize a MomentsAccountant.\n",
        "\n",
        "    Args:\n",
        "      total_examples: total number of examples.\n",
        "      moment_orders: the order of moments to keep.\n",
        "    \"\"\"\n",
        "\n",
        "    assert total_examples > 0\n",
        "    self._total_examples = total_examples\n",
        "    self._moment_orders = (moment_orders\n",
        "                           if isinstance(moment_orders, (list, tuple))\n",
        "                           else range(1, moment_orders + 1))\n",
        "    self._max_moment_order = max(self._moment_orders)\n",
        "    assert self._max_moment_order < 100, \"The moment order is too large.\"\n",
        "    self._log_moments = [tf.Variable(numpy.float64(0.0),\n",
        "                                     trainable=False,\n",
        "                                     name=(\"log_moments-%d\" % moment_order))\n",
        "                         for moment_order in self._moment_orders]\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _compute_log_moment(self, sigma, q, moment_order):\n",
        "    \"\"\"Compute high moment of privacy loss.\n",
        "\n",
        "    Args:\n",
        "      sigma: the noise sigma, in the multiples of the sensitivity.\n",
        "      q: the sampling ratio.\n",
        "      moment_order: the order of moment.\n",
        "    Returns:\n",
        "      log E[exp(moment_order * X)]\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def accumulate_privacy_spending(self, unused_eps_delta,\n",
        "                                  sigma, num_examples):\n",
        "    \"\"\"Accumulate privacy spending.\n",
        "\n",
        "    In particular, accounts for privacy spending when we assume there\n",
        "    are num_examples, and we are releasing the vector\n",
        "    (sum_{i=1}^{num_examples} x_i) + Normal(0, stddev=l2norm_bound*sigma)\n",
        "    where l2norm_bound is the maximum l2_norm of each example x_i, and\n",
        "    the num_examples have been randomly selected out of a pool of\n",
        "    self.total_examples.\n",
        "\n",
        "    Args:\n",
        "      unused_eps_delta: EpsDelta pair which can be tensors. Unused\n",
        "        in this accountant.\n",
        "      sigma: the noise sigma, in the multiples of the sensitivity (that is,\n",
        "        if the l2norm sensitivity is k, then the caller must have added\n",
        "        Gaussian noise with stddev=k*sigma to the result of the query).\n",
        "      num_examples: the number of examples involved.\n",
        "    Returns:\n",
        "      a TensorFlow operation for updating the privacy spending.\n",
        "    \"\"\"\n",
        "    q = tf.cast(num_examples, tf.float64) * 1.0 / self._total_examples\n",
        "\n",
        "    moments_accum_ops = []\n",
        "    for i in range(len(self._log_moments)):\n",
        "      moment = self._compute_log_moment(sigma, q, self._moment_orders[i])\n",
        "      moments_accum_ops.append(tf.assign_add(self._log_moments[i], moment))\n",
        "    return tf.group(*moments_accum_ops)\n",
        "\n",
        "  def _compute_delta(self, log_moments, eps):\n",
        "    \"\"\"Compute delta for given log_moments and eps.\n",
        "\n",
        "    Args:\n",
        "      log_moments: the log moments of privacy loss, in the form of pairs\n",
        "        of (moment_order, log_moment)\n",
        "      eps: the target epsilon.\n",
        "    Returns:\n",
        "      delta\n",
        "    \"\"\"\n",
        "    min_delta = 1.0\n",
        "    for moment_order, log_moment in log_moments:\n",
        "      if math.isinf(log_moment) or math.isnan(log_moment):\n",
        "        sys.stderr.write(\"The %d-th order is inf or Nan\\n\" % moment_order)\n",
        "        continue\n",
        "      if log_moment < moment_order * eps:\n",
        "        min_delta = min(min_delta,\n",
        "                        math.exp(log_moment - moment_order * eps))\n",
        "    return min_delta\n",
        "\n",
        "  def _compute_eps(self, log_moments, delta):\n",
        "    min_eps = float(\"inf\")\n",
        "    for moment_order, log_moment in log_moments:\n",
        "      if math.isinf(log_moment) or math.isnan(log_moment):\n",
        "        sys.stderr.write(\"The %d-th order is inf or Nan\\n\" % moment_order)\n",
        "        continue\n",
        "      min_eps = min(min_eps, (log_moment - math.log(delta)) / moment_order)\n",
        "    return min_eps\n",
        "\n",
        "  def get_privacy_spent(self, sess, target_eps=None, target_deltas=None):\n",
        "    \"\"\"Compute privacy spending in (e, d)-DP form for a single or list of eps.\n",
        "\n",
        "    Args:\n",
        "      sess: the session to run the tensor.\n",
        "      target_eps: a list of target epsilon's for which we would like to\n",
        "        compute corresponding delta value.\n",
        "      target_deltas: a list of target deltas for which we would like to\n",
        "        compute the corresponding eps value. Caller must specify\n",
        "        either target_eps or target_delta.\n",
        "    Returns:\n",
        "      A list of EpsDelta pairs.\n",
        "    \"\"\"\n",
        "    assert (target_eps is None) ^ (target_deltas is None)\n",
        "    eps_deltas = []\n",
        "    log_moments = sess.run(self._log_moments)\n",
        "    log_moments_with_order = zip(self._moment_orders, log_moments)\n",
        "    if target_eps is not None:\n",
        "      for eps in target_eps:\n",
        "        eps_deltas.append(\n",
        "            EpsDelta(eps, self._compute_delta(log_moments_with_order, eps)))\n",
        "    else:\n",
        "      assert target_deltas\n",
        "      for delta in target_deltas:\n",
        "        eps_deltas.append(\n",
        "            EpsDelta(self._compute_eps(log_moments_with_order, delta), delta))\n",
        "    return eps_deltas\n",
        "\n",
        "\n",
        "class GaussianMomentsAccountant(MomentsAccountant):\n",
        "  \"\"\"MomentsAccountant which assumes Gaussian noise.\n",
        "\n",
        "  GaussianMomentsAccountant assumes the noise added is centered Gaussian\n",
        "  noise N(0, sigma^2 I). In this case, we can compute the differential moments\n",
        "  accurately using a formula.\n",
        "\n",
        "  For asymptotic bound, for Gaussian noise with variance sigma^2, we can show\n",
        "  for L < sigma^2,  q L < sigma,\n",
        "    log E[exp(L X)] = O(q^2 L^2 / sigma^2).\n",
        "  Using this we derive that for training T epoches, with batch ratio q,\n",
        "  the Gaussian mechanism with variance sigma^2 (with q < 1/sigma) is (e, d)\n",
        "  private for d = exp(T/q q^2 L^2 / sigma^2 - L e). Setting L = sigma^2,\n",
        "  Tq = e/2, the mechanism is (e, exp(-e sigma^2/2))-DP. Equivalently, the\n",
        "  mechanism is (e, d)-DP if sigma = sqrt{2 log(1/d)}/e, q < 1/sigma,\n",
        "  and T < e/(2q). This bound is better than the bound obtained using general\n",
        "  composition theorems, by an Omega(sqrt{log k}) factor on epsilon, if we run\n",
        "  k steps. Since we use direct estimate, the obtained privacy bound has tight\n",
        "  constant.\n",
        "\n",
        "  For GaussianMomentAccountant, it suffices to compute I1, as I1 >= I2,\n",
        "  which reduce to computing E(P(x+s)/P(x+s-1) - 1)^i for s = 0 and 1. In the\n",
        "  companion gaussian_moments.py file, we supply procedure for computing both\n",
        "  I1 and I2 (the computation of I2 is through multi-precision integration\n",
        "  package). It can be verified that indeed I1 >= I2 for wide range of parameters\n",
        "  we have tried, though at the moment we are unable to prove this claim.\n",
        "\n",
        "  We recommend that when using this accountant, users independently verify\n",
        "  using gaussian_moments.py that for their parameters, I1 is indeed larger\n",
        "  than I2. This can be done by following the instructions in\n",
        "  gaussian_moments.py.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, total_examples, moment_orders=32):\n",
        "    \"\"\"Initialization.\n",
        "\n",
        "    Args:\n",
        "      total_examples: total number of examples.\n",
        "      moment_orders: the order of moments to keep.\n",
        "    \"\"\"\n",
        "    super(self.__class__, self).__init__(total_examples, moment_orders)\n",
        "    self._binomial_table = GenerateBinomialTable(self._max_moment_order)\n",
        "\n",
        "  def _differential_moments(self, sigma, s, t):\n",
        "    \"\"\"Compute 0 to t-th differential moments for Gaussian variable.\n",
        "\n",
        "        E[(P(x+s)/P(x+s-1)-1)^t]\n",
        "      = sum_{i=0}^t (t choose i) (-1)^{t-i} E[(P(x+s)/P(x+s-1))^i]\n",
        "      = sum_{i=0}^t (t choose i) (-1)^{t-i} E[exp(-i*(2*x+2*s-1)/(2*sigma^2))]\n",
        "      = sum_{i=0}^t (t choose i) (-1)^{t-i} exp(i(i+1-2*s)/(2 sigma^2))\n",
        "    Args:\n",
        "      sigma: the noise sigma, in the multiples of the sensitivity.\n",
        "      s: the shift.\n",
        "      t: 0 to t-th moment.\n",
        "    Returns:\n",
        "      0 to t-th moment as a tensor of shape [t+1].\n",
        "    \"\"\"\n",
        "    assert t <= self._max_moment_order, (\"The order of %d is out \"\n",
        "                                         \"of the upper bound %d.\"\n",
        "                                         % (t, self._max_moment_order))\n",
        "    binomial = tf.slice(self._binomial_table, [0, 0],\n",
        "                        [t + 1, t + 1])\n",
        "    signs = numpy.zeros((t + 1, t + 1), dtype=numpy.float64)\n",
        "    for i in range(t + 1):\n",
        "      for j in range(t + 1):\n",
        "        signs[i, j] = 1.0 - 2 * ((i - j) % 2)\n",
        "    exponents = tf.constant([j * (j + 1.0 - 2.0 * s) / (2.0 * sigma * sigma)\n",
        "                             for j in range(t + 1)], dtype=tf.float64)\n",
        "    # x[i, j] = binomial[i, j] * signs[i, j] = (i choose j) * (-1)^{i-j}\n",
        "    x = tf.multiply(binomial, signs)\n",
        "    # y[i, j] = x[i, j] * exp(exponents[j])\n",
        "    #         = (i choose j) * (-1)^{i-j} * exp(j(j-1)/(2 sigma^2))\n",
        "    # Note: this computation is done by broadcasting pointwise multiplication\n",
        "    # between [t+1, t+1] tensor and [t+1] tensor.\n",
        "    y = tf.multiply(x, tf.exp(exponents))\n",
        "    # z[i] = sum_j y[i, j]\n",
        "    #      = sum_j (i choose j) * (-1)^{i-j} * exp(j(j-1)/(2 sigma^2))\n",
        "    z = tf.reduce_sum(y, 1)\n",
        "    return z\n",
        "\n",
        "  def _compute_log_moment(self, sigma, q, moment_order):\n",
        "    \"\"\"Compute high moment of privacy loss.\n",
        "\n",
        "    Args:\n",
        "      sigma: the noise sigma, in the multiples of the sensitivity.\n",
        "      q: the sampling ratio.\n",
        "      moment_order: the order of moment.\n",
        "    Returns:\n",
        "      log E[exp(moment_order * X)]\n",
        "    \"\"\"\n",
        "    assert moment_order <= self._max_moment_order, (\"The order of %d is out \"\n",
        "                                                    \"of the upper bound %d.\"\n",
        "                                                    % (moment_order,\n",
        "                                                       self._max_moment_order))\n",
        "    binomial_table = tf.slice(self._binomial_table, [moment_order, 0],\n",
        "                              [1, moment_order + 1])\n",
        "    # qs = [1 q q^2 ... q^L] = exp([0 1 2 ... L] * log(q))\n",
        "    qs = tf.exp(tf.constant([i * 1.0 for i in range(moment_order + 1)],\n",
        "                            dtype=tf.float64) * tf.cast(\n",
        "                                tf.log(q), dtype=tf.float64))\n",
        "    moments0 = self._differential_moments(sigma, 0.0, moment_order)\n",
        "    term0 = tf.reduce_sum(binomial_table * qs * moments0)\n",
        "    moments1 = self._differential_moments(sigma, 1.0, moment_order)\n",
        "    term1 = tf.reduce_sum(binomial_table * qs * moments1)\n",
        "    return tf.squeeze(tf.log(tf.cast(q * term0 + (1.0 - q) * term1,\n",
        "                                     tf.float64)))\n",
        "\n",
        "\n",
        "class DummyAccountant(object):\n",
        "  \"\"\"An accountant that does no accounting.\"\"\"\n",
        "\n",
        "  def accumulate_privacy_spending(self, *unused_args):\n",
        "    return tf.no_op()\n",
        "\n",
        "  def get_privacy_spent(self, unused_sess, **unused_kwargs):\n",
        "    return [EpsDelta(numpy.inf, 1.0)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_E7eX297V4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Our DP-CGAN (main)"
      ]
    },
    {
      "metadata": {
        "id": "pOTLXdxe7PWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "128ad4f6-cbd8-4e81-a6c3-0815692fa5fa"
      },
      "cell_type": "code",
      "source": [
        "from mlxtend.data import loadlocal_mnist\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Import the requiered python packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "# Import required Differential Privacy packages\n",
        "baseDir = \"/content/gdrive/Team Drives/PrivacyGenomics/\"\n",
        "baseDir = \"../\"\n",
        "sys.path.append(baseDir);\n",
        "\n",
        "#from differential_privacy.dp_sgd.dp_optimizer import dp_optimizer\n",
        "#from differential_privacy.dp_sgd.dp_optimizer import sanitizer\n",
        "#from differential_privacy.dp_sgd.dp_optimizer import utils\n",
        "#from differential_privacy.privacy_accountant.tf import accountant\n",
        "\n",
        "\n",
        "sigmaList = [ 2]\n",
        "batchSizeList = [600]\n",
        "clippingValueList = [2]\n",
        "epsilon = 8\n",
        "delta = 1e-5\n",
        "\n",
        "def compute_fpr_tpr_roc(Y_test, Y_score):\n",
        "    n_classes = Y_score.shape[1]\n",
        "    false_positive_rate = dict()\n",
        "    true_positive_rate = dict()\n",
        "    roc_auc = dict()\n",
        "    for class_cntr in range(n_classes):\n",
        "        false_positive_rate[class_cntr], true_positive_rate[class_cntr], _ = roc_curve(Y_test[:, class_cntr],\n",
        "                                                                                       Y_score[:, class_cntr])\n",
        "        roc_auc[class_cntr] = auc(false_positive_rate[class_cntr], true_positive_rate[class_cntr])\n",
        "\n",
        "    # Compute micro-average ROC curve and ROC area\n",
        "    false_positive_rate[\"micro\"], true_positive_rate[\"micro\"], _ = roc_curve(Y_test.ravel(), Y_score.ravel())\n",
        "    roc_auc[\"micro\"] = auc(false_positive_rate[\"micro\"], true_positive_rate[\"micro\"])\n",
        "\n",
        "    return false_positive_rate, true_positive_rate, roc_auc\n",
        "\n",
        "\n",
        "def classify(X_train, Y_train, X_test, classiferName, random_state_value=0):\n",
        "    if classiferName == \"svm\":\n",
        "        classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state_value))\n",
        "    elif classiferName == \"dt\":\n",
        "        classifier = OneVsRestClassifier(DecisionTreeClassifier(random_state=random_state_value))\n",
        "    elif classiferName == \"lr\":\n",
        "        classifier = OneVsRestClassifier(\n",
        "            LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=random_state_value))\n",
        "    elif classiferName == \"rf\":\n",
        "        classifier = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=random_state_value))\n",
        "    elif classiferName == \"gnb\":\n",
        "        classifier = OneVsRestClassifier(GaussianNB())\n",
        "    elif classiferName == \"bnb\":\n",
        "        classifier = OneVsRestClassifier(BernoulliNB(alpha=.01))\n",
        "    elif classiferName == \"ab\":\n",
        "        classifier = OneVsRestClassifier(AdaBoostClassifier(random_state=random_state_value))\n",
        "    elif classiferName == \"mlp\":\n",
        "        classifier = OneVsRestClassifier(MLPClassifier(random_state=random_state_value, alpha=1))\n",
        "    else:\n",
        "        print(\"Classifier not in the list!\")\n",
        "        exit()\n",
        "\n",
        "    Y_score = classifier.fit(X_train, Y_train).predict_proba(X_test)\n",
        "    return Y_score\n",
        "\n",
        "def xavier_init(size):\n",
        "    \"\"\" Xavier Function to keep the scale of the gradients roughly the same\n",
        "        in all the layers.\n",
        "    \"\"\"\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
        "\n",
        "\n",
        "def sample_Z(m, n):\n",
        "    \"\"\" Function to generate uniform prior for G(z)\n",
        "    \"\"\"\n",
        "    return np.random.uniform(-1., 1., size=[m, n])\n",
        "\n",
        "\n",
        "def generator(z, y, theta_G):\n",
        "    G_W1 = theta_G[0]\n",
        "    G_W2 = theta_G[1]\n",
        "    G_b1 = theta_G[2]\n",
        "    G_b2 = theta_G[3]\n",
        "\n",
        "    \"\"\" Function to build the generator network\n",
        "    \"\"\"\n",
        "    inputs = tf.concat(axis=1, values=[z, y])\n",
        "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
        "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
        "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
        "    return G_prob\n",
        "\n",
        "\n",
        "def discriminator(x, y, theta_D):\n",
        "    \"\"\" Function to build the discriminator network\n",
        "    \"\"\"\n",
        "    D_W1 = theta_D[0]\n",
        "    D_W2 = theta_D[1]\n",
        "    D_b1 = theta_D[2]\n",
        "    D_b2 = theta_D[3]\n",
        "\n",
        "    inputs = tf.concat(axis=1, values=[x, y])\n",
        "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
        "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
        "    D_prob = tf.nn.sigmoid(D_logit)\n",
        "\n",
        "    return D_prob, D_logit\n",
        "\n",
        "\n",
        "def plot(samples):\n",
        "    \"\"\" Function to plot the generated images\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    gs = gridspec.GridSpec(10, 1)\n",
        "    gs.update(wspace=0.05, hspace=0.05)\n",
        "    for i, sample in enumerate(samples):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect('equal')\n",
        "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "        # plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    \"\"\" Function to delete all flags before declare\n",
        "    \"\"\"\n",
        "    flags_dict = FLAGS._flags()\n",
        "    keys_list = [keys for keys in flags_dict]\n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "\n",
        "\n",
        "def runTensorFlow(sigma, clippingValue, batchSize, epsilon, delta):\n",
        "    h_dim = 128\n",
        "    Z_dim = 100\n",
        "\n",
        "    # Initializations for a two-layer discriminator network\n",
        "    mnist = input_data.read_data_sets(baseDir + \"conditional-gan-dp-ours-mnist/mnist_dataset\", one_hot=True)\n",
        "    X_dim = mnist.train.images.shape[1]\n",
        "    y_dim = mnist.train.labels.shape[1]\n",
        "    X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
        "    y = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
        "\n",
        "\n",
        "    D_W1 = tf.Variable(xavier_init([X_dim + y_dim, h_dim]))\n",
        "    D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
        "    D_W2 = tf.Variable(xavier_init([h_dim, 1]))\n",
        "    D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
        "\n",
        "    theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
        "\n",
        "    # Initializations for a two-layer genrator network\n",
        "    Z = tf.placeholder(tf.float32, shape=[None, Z_dim])\n",
        "    G_W1 = tf.Variable(xavier_init([Z_dim + y_dim, h_dim]))\n",
        "    G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
        "    G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
        "    G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
        "    theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
        "\n",
        "    # Delete all Flags\n",
        "    del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "    # Set training parameters\n",
        "    tf.flags.DEFINE_string('f', '', 'kernel')\n",
        "    tf.flags.DEFINE_float(\"lr\", 0.05, \"start learning rate\")\n",
        "    tf.flags.DEFINE_float(\"end_lr\", 0.05, \"end learning rate\")\n",
        "    tf.flags.DEFINE_float(\"lr_saturate_epochs\", 0,\n",
        "                          \"learning rate saturate epochs; set to 0 for a constant\"\n",
        "                          \"learning rate of --lr.\")\n",
        "    tf.flags.DEFINE_integer(\"batch_size\", batchSize, \"The training batch size.\")\n",
        "    tf.flags.DEFINE_integer(\"batches_per_lot\", 1, \"Number of batches per lot.\")\n",
        "    tf.flags.DEFINE_integer(\"num_training_steps\", 100000, \"The number of training\"\n",
        "                                                          \"steps. This counts number of lots.\")\n",
        "\n",
        "    # Flags that control privacy spending during training\n",
        "    tf.flags.DEFINE_float(\"target_delta\", delta, \"Maximum delta for\"\n",
        "                                                \"--terminate_based_on_privacy.\")\n",
        "    tf.flags.DEFINE_float(\"sigma\", sigma, \"Noise sigma, used only if accountant_type\"\n",
        "                                      \"is Moments\")\n",
        "    tf.flags.DEFINE_string(\"target_eps\", str(epsilon),\n",
        "                           \"Log the privacy loss for the target epsilon's. Only\"\n",
        "                           \"used when accountant_type is Moments.\")\n",
        "    tf.flags.DEFINE_float(\"default_gradient_l2norm_bound\", clippingValue, \"norm clipping\")\n",
        "\n",
        "    FLAGS = tf.flags.FLAGS\n",
        "\n",
        "    # Set accountant type to GaussianMomentsAccountant\n",
        "    NUM_TRAINING_IMAGES = 60000\n",
        "    priv_accountant = GaussianMomentsAccountant(NUM_TRAINING_IMAGES)\n",
        "\n",
        "    # Sanitizer\n",
        "    batch_size = FLAGS.batch_size\n",
        "    clipping_value = FLAGS.default_gradient_l2norm_bound\n",
        "    #gaussian_sanitizer = sanitizer.AmortizedGaussianSanitizer(priv_accountant,\n",
        "    #                                                          [clipping_value / batch_size, True])\n",
        "    gaussian_sanitizer = AmortizedGaussianSanitizer(priv_accountant,\n",
        "                                                              [clipping_value / batch_size, True])\n",
        "\n",
        "    # Instantiate the Generator Network\n",
        "    G_sample = generator(Z, y, theta_G)\n",
        "\n",
        "    # Instantiate the Discriminator Network\n",
        "    D_real, D_logit_real = discriminator(X, y, theta_D)\n",
        "    D_fake, D_logit_fake = discriminator(G_sample, y, theta_D)\n",
        "\n",
        "    # Discriminator loss for real data\n",
        "    D_loss_real = tf.reduce_mean(\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits( \\\n",
        "            logits=D_logit_real, \\\n",
        "            labels=tf.ones_like(D_logit_real)), \\\n",
        "        [0])\n",
        "    # Discriminator loss for fake data\n",
        "    D_loss_fake = tf.reduce_mean( \\\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits( \\\n",
        "            logits=D_logit_fake, \\\n",
        "            labels=tf.zeros_like(D_logit_fake)), [0])\n",
        "\n",
        "    # Generator loss\n",
        "    G_loss = tf.reduce_mean( \\\n",
        "        tf.nn.sigmoid_cross_entropy_with_logits( \\\n",
        "            logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)) \\\n",
        "        , [0])\n",
        "\n",
        "    # Generator optimizer\n",
        "    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
        "\n",
        "    # Discriminator Optimizer\n",
        "    # ------------------------------------------------------------------------------\n",
        "    \"\"\"\n",
        "    minimize_ours :\n",
        "            Our method (Clipping the gradients of loss on real data and making\n",
        "            them noisy + Clipping the gradients of loss on fake data) is\n",
        "            implemented in this function .\n",
        "            It can be found in the following directory:\n",
        "            /content/gdrive/Team Drives/PrivacyGenomics/our_dp_gan/\n",
        "            differential_privacy/dp_sgd/dp_optimizer/dp_optimizer.py'\n",
        "    \"\"\"\n",
        "    lr = tf.placeholder(tf.float32)\n",
        "    sigma = FLAGS.sigma\n",
        "    D_solver = DPGradientDescentOptimizer( \\\n",
        "        lr, [None, None], \\\n",
        "        gaussian_sanitizer, \\\n",
        "        sigma=sigma, \\\n",
        "        batches_per_lot= \\\n",
        "            FLAGS.batches_per_lot). \\\n",
        "        minimize_ours( \\\n",
        "        D_loss_real, \\\n",
        "        D_loss_fake, \\\n",
        "        var_list=theta_D)\n",
        "    # ------------------------------------------------------------------------------\n",
        "\n",
        "    # Set output directory\n",
        "    resultDir = baseDir + \"conditional-gan-dp-ours-mnist/results\"\n",
        "    if not os.path.exists(resultDir):\n",
        "        os.makedirs(resultDir)\n",
        "\n",
        "    resultPath = resultDir + \"/bs_{}_s_{}_c_{}_d_{}_e_{}\".format( \\\n",
        "        batch_size, \\\n",
        "        sigma, \\\n",
        "        clipping_value, \\\n",
        "        FLAGS.target_delta, FLAGS.target_eps)\n",
        "\n",
        "    if not os.path.exists(resultPath):\n",
        "        os.makedirs(resultPath)\n",
        "\n",
        "    target_eps = [float(s) for s in FLAGS.target_eps.split(\",\")]\n",
        "    max_target_eps = max(target_eps)\n",
        "\n",
        "    # Main Session\n",
        "    with tf.Session() as sess:\n",
        "        init = tf.initialize_all_variables()\n",
        "        sess.run(init)\n",
        "\n",
        "        lot_size = FLAGS.batches_per_lot * batch_size\n",
        "        lots_per_epoch = NUM_TRAINING_IMAGES / lot_size\n",
        "        step = 0\n",
        "\n",
        "        # Is true when the spent privacy budget exceeds the target budget\n",
        "        should_terminate = False\n",
        "\n",
        "        # Main loop\n",
        "        while (step < FLAGS.num_training_steps and should_terminate == False):\n",
        "\n",
        "            epoch = step / lots_per_epoch\n",
        "            curr_lr = VaryRate(FLAGS.lr, FLAGS.end_lr, \\\n",
        "                                     FLAGS.lr_saturate_epochs, epoch)\n",
        "\n",
        "            for _ in range(FLAGS.batches_per_lot):\n",
        "\n",
        "                # Save the generated images every 100 steps\n",
        "                if step % 100 == 0:\n",
        "                    n_sample = 10\n",
        "                    Z_sample = sample_Z(n_sample, Z_dim)\n",
        "                    y_sample = np.zeros(shape=[n_sample, y_dim])\n",
        "\n",
        "                    y_sample[0, 0] = 1\n",
        "                    y_sample[1, 1] = 1\n",
        "                    y_sample[2, 2] = 1\n",
        "                    y_sample[3, 3] = 1\n",
        "                    y_sample[4, 4] = 1\n",
        "                    y_sample[5, 5] = 1\n",
        "                    y_sample[6, 6] = 1\n",
        "                    y_sample[7, 7] = 1\n",
        "                    y_sample[8, 8] = 1\n",
        "                    y_sample[9, 9] = 1\n",
        "\n",
        "\n",
        "                    samples = sess.run(G_sample, feed_dict={Z: Z_sample, y: y_sample})\n",
        "\n",
        "                    fig = plot(samples)\n",
        "                    plt.savefig(\n",
        "                        (resultPath + \"/step_{}.png\").format(str(step).zfill(3)), bbox_inches='tight')\n",
        "                    plt.close(fig)\n",
        "\n",
        "\n",
        "                X_mb, y_mb = mnist.train.next_batch(batch_size, shuffle=True)\n",
        "\n",
        "                Z_sample = sample_Z(batch_size, Z_dim)\n",
        "\n",
        "                # Update the discriminator network\n",
        "                _, D_loss_real_curr, D_loss_fake_curr= sess.run([D_solver, D_loss_real, D_loss_fake], \\\n",
        "                                             feed_dict={X: X_mb, \\\n",
        "                                                        Z: Z_sample, \\\n",
        "                                                        y: y_mb, \\\n",
        "                                                        lr: curr_lr})\n",
        "                # Update the generator network\n",
        "                _, G_loss_curr = sess.run([G_solver, G_loss],\n",
        "                                          feed_dict={Z: Z_sample, y: y_mb})\n",
        "\n",
        "            # Flag to terminate based on target privacy budget\n",
        "            terminate_spent_eps_delta = priv_accountant.get_privacy_spent(sess, \\\n",
        "                                                                          target_eps=[max_target_eps])[0]\n",
        "\n",
        "            # For the Moments accountant, we should always have \\\n",
        "            # spent_eps == max_target_eps.\n",
        "            if (terminate_spent_eps_delta.spent_delta > FLAGS.target_delta or \\\n",
        "                            terminate_spent_eps_delta.spent_eps > max_target_eps):\n",
        "                spent_eps_deltas = priv_accountant.get_privacy_spent( \\\n",
        "                    sess, target_eps=target_eps)\n",
        "                print(\"TERMINATE!!!!\")\n",
        "                print(\"Termination Step : \" + str(step))\n",
        "                should_terminate = True\n",
        "\n",
        "                n_class = np.zeros(10)\n",
        "\n",
        "                n_class[0] = 5923\n",
        "                n_class[1] = 6742\n",
        "                n_class[2] = 5958\n",
        "                n_class[3] = 6131\n",
        "                n_class[4] = 5842\n",
        "                n_class[5] = 5421\n",
        "                n_class[6] = 5918\n",
        "                n_class[7] = 6265\n",
        "                n_class[8] = 5851\n",
        "                n_class[9] = 5949\n",
        "\n",
        "                n_image = int(sum(n_class))\n",
        "                image_lables = np.zeros(shape=[n_image, len(n_class)])\n",
        "\n",
        "                image_cntr = 0\n",
        "                for class_cntr in np.arange(len(n_class)):\n",
        "                    for cntr in np.arange(n_class[class_cntr]):\n",
        "                        image_lables[image_cntr, class_cntr] = 1\n",
        "                        image_cntr += 1\n",
        "\n",
        "                Z_sample = sample_Z(n_image, Z_dim)\n",
        "\n",
        "                images = sess.run(G_sample, feed_dict={Z: Z_sample, y: image_lables})\n",
        "\n",
        "                X_test, Y_test = loadlocal_mnist(\n",
        "                    images_path=baseDir + 'mnist_dataset/t10k-images.idx3-ubyte',\n",
        "                    labels_path=baseDir + 'mnist_dataset/t10k-labels.idx1-ubyte')\n",
        "\n",
        "                Y_test = [int(y) for y in Y_test]\n",
        "\n",
        "                # X = X.reshape((X.shape[0], -1))\n",
        "\n",
        "                print(\"Binarizing the labels ...\")\n",
        "                classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "                Y_test = label_binarize(Y_test, classes=classes)\n",
        "\n",
        "                print(\"\\n################# Logistic Regression #######################\")\n",
        "\n",
        "                print(\"  Classifying ...\")\n",
        "                Y_score = classify(images, image_lables, X_test, \"lr\", random_state_value=30)\n",
        "\n",
        "                print(\"  Computing ROC ...\")\n",
        "                false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(Y_test, Y_score)\n",
        "                print(\"  AUROC: \" + str(roc_auc[\"micro\"]))\n",
        "\n",
        "                print(\"\\n################# Random Forest #######################\")\n",
        "\n",
        "                print(\"  Classifying ...\")\n",
        "                Y_score = classify(images, image_lables, X_test, \"rf\", random_state_value=30)\n",
        "\n",
        "                print(\"  Computing ROC ...\")\n",
        "                false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(Y_test, Y_score)\n",
        "                print(\"  AUROC: \" + str(roc_auc[\"micro\"]))\n",
        "\n",
        "                print(\"\\n################# Gaussian Naive Bayes #######################\")\n",
        "\n",
        "                print(\"  Classifying ...\")\n",
        "                Y_score = classify(images, image_lables, X_test, \"gnb\", random_state_value=30)\n",
        "\n",
        "                print(\"  Computing ROC ...\")\n",
        "                false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(Y_test, Y_score)\n",
        "                print(\"  AUROC: \" + str(roc_auc[\"micro\"]))\n",
        "\n",
        "                print(\"\\n################# Decision Tree #######################\")\n",
        "\n",
        "                print(\"  Classifying ...\")\n",
        "                Y_score = classify(images, image_lables, X_test, \"dt\", random_state_value=30)\n",
        "\n",
        "                print(\"  Computing ROC ...\")\n",
        "                false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(Y_test, Y_score)\n",
        "                print(\"  AUROC: \" + str(roc_auc[\"micro\"]))\n",
        "\n",
        "                print(\"\\n################# Multi-layer Perceptron #######################\")\n",
        "\n",
        "                print(\"  Classifying ...\")\n",
        "                Y_score = classify(images, image_lables, X_test, \"mlp\", random_state_value=30)\n",
        "\n",
        "                print(\"  Computing ROC ...\")\n",
        "                false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(Y_test, Y_score)\n",
        "                print(\"  AUROC: \" + str(roc_auc[\"micro\"]))\n",
        "\n",
        "            step = step + 1\n",
        "\n",
        "\n",
        "for sigma in sigmaList:\n",
        "    for clippingValue in clippingValueList:\n",
        "        for batchSize in batchSizeList:\n",
        "            print (\"Running TensorFlow with Sigma=%f, Clipping=%d, batchSize=%d\\n\" % (sigma, clippingValue, batchSize))\n",
        "            runTensorFlow(sigma, float(clippingValue), batchSize, epsilon, delta)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running TensorFlow with Sigma=2.000000, Clipping=2, batchSize=600\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-7-7d85505fb6bb>:162: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../conditional-gan-dp-ours-mnist/mnist_dataset/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../conditional-gan-dp-ours-mnist/mnist_dataset/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting ../conditional-gan-dp-ours-mnist/mnist_dataset/t10k-images-idx3-ubyte.gz\n",
            "Extracting ../conditional-gan-dp-ours-mnist/mnist_dataset/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}