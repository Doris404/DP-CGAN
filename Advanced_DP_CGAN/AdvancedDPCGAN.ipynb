{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdvancedDPCGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "w4uknVSySjrQ",
        "RPcYboVZSO4G",
        "kh060foYSIxD"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reihaneh-torkzadehmahani/MyDPGAN/blob/master/AdvancedDPCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "iCAPo5dqWGdy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## differential_privacy.analysis.rdp_accountant"
      ]
    },
    {
      "metadata": {
        "id": "yD6cpGQUWDYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"RDP analysis of the Sampled Gaussian Mechanism.\n",
        "\n",
        "Functionality for computing Renyi differential privacy (RDP) of an additive\n",
        "Sampled Gaussian Mechanism (SGM). Its public interface consists of two methods:\n",
        "  compute_rdp(q, noise_multiplier, T, orders) computes RDP for SGM iterated\n",
        "                                   T times.\n",
        "  get_privacy_spent(orders, rdp, target_eps, target_delta) computes delta\n",
        "                                   (or eps) given RDP at multiple orders and\n",
        "                                   a target value for eps (or delta).\n",
        "\n",
        "Example use:\n",
        "\n",
        "Suppose that we have run an SGM applied to a function with l2-sensitivity 1.\n",
        "Its parameters are given as a list of tuples (q1, sigma1, T1), ...,\n",
        "(qk, sigma_k, Tk), and we wish to compute eps for a given delta.\n",
        "The example code would be:\n",
        "\n",
        "  max_order = 32\n",
        "  orders = range(2, max_order + 1)\n",
        "  rdp = np.zeros_like(orders, dtype=float)\n",
        "  for q, sigma, T in parameters:\n",
        "   rdp += rdp_accountant.compute_rdp(q, sigma, T, orders)\n",
        "  eps, _, opt_order = rdp_accountant.get_privacy_spent(rdp, target_delta=delta)\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n",
        "import six\n",
        "\n",
        "########################\n",
        "# LOG-SPACE ARITHMETIC #\n",
        "########################\n",
        "\n",
        "\n",
        "def _log_add(logx, logy):\n",
        "    \"\"\"Add two numbers in the log space.\"\"\"\n",
        "    a, b = min(logx, logy), max(logx, logy)\n",
        "    if a == -np.inf:  # adding 0\n",
        "        return b\n",
        "    # Use exp(a) + exp(b) = (exp(a - b) + 1) * exp(b)\n",
        "    return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)\n",
        "\n",
        "\n",
        "def _log_sub(logx, logy):\n",
        "    \"\"\"Subtract two numbers in the log space. Answer must be non-negative.\"\"\"\n",
        "    if logx < logy:\n",
        "        raise ValueError(\"The result of subtraction must be non-negative.\")\n",
        "    if logy == -np.inf:  # subtracting 0\n",
        "        return logx\n",
        "    if logx == logy:\n",
        "        return -np.inf  # 0 is represented as -np.inf in the log space.\n",
        "\n",
        "    try:\n",
        "        # Use exp(x) - exp(y) = (exp(x - y) - 1) * exp(y).\n",
        "        return math.log(\n",
        "            math.expm1(logx - logy)) + logy  # expm1(x) = exp(x) - 1\n",
        "    except OverflowError:\n",
        "        return logx\n",
        "\n",
        "\n",
        "def _log_print(logx):\n",
        "    \"\"\"Pretty print.\"\"\"\n",
        "    if logx < math.log(sys.float_info.max):\n",
        "        return \"{}\".format(math.exp(logx))\n",
        "    else:\n",
        "        return \"exp({})\".format(logx)\n",
        "\n",
        "\n",
        "def _compute_log_a_int(q, sigma, alpha):\n",
        "    \"\"\"Compute log(A_alpha) for integer alpha. 0 < q < 1.\"\"\"\n",
        "    assert isinstance(alpha, six.integer_types)\n",
        "\n",
        "    # Initialize with 0 in the log space.\n",
        "    log_a = -np.inf\n",
        "\n",
        "    for i in range(alpha + 1):\n",
        "        log_coef_i = (math.log(special.binom(alpha, i)) + i * math.log(q) +\n",
        "                      (alpha - i) * math.log(1 - q))\n",
        "\n",
        "        s = log_coef_i + (i * i - i) / (2 * (sigma**2))\n",
        "        log_a = _log_add(log_a, s)\n",
        "\n",
        "    return float(log_a)\n",
        "\n",
        "\n",
        "def _compute_log_a_frac(q, sigma, alpha):\n",
        "    \"\"\"Compute log(A_alpha) for fractional alpha. 0 < q < 1.\"\"\"\n",
        "    # The two parts of A_alpha, integrals over (-inf,z0] and [z0, +inf), are\n",
        "    # initialized to 0 in the log space:\n",
        "    log_a0, log_a1 = -np.inf, -np.inf\n",
        "    i = 0\n",
        "\n",
        "    z0 = sigma**2 * math.log(1 / q - 1) + .5\n",
        "\n",
        "    while True:  # do ... until loop\n",
        "        coef = special.binom(alpha, i)\n",
        "        log_coef = math.log(abs(coef))\n",
        "        j = alpha - i\n",
        "\n",
        "        log_t0 = log_coef + i * math.log(q) + j * math.log(1 - q)\n",
        "        log_t1 = log_coef + j * math.log(q) + i * math.log(1 - q)\n",
        "\n",
        "        log_e0 = math.log(.5) + _log_erfc((i - z0) / (math.sqrt(2) * sigma))\n",
        "        log_e1 = math.log(.5) + _log_erfc((z0 - j) / (math.sqrt(2) * sigma))\n",
        "\n",
        "        log_s0 = log_t0 + (i * i - i) / (2 * (sigma**2)) + log_e0\n",
        "        log_s1 = log_t1 + (j * j - j) / (2 * (sigma**2)) + log_e1\n",
        "\n",
        "        if coef > 0:\n",
        "            log_a0 = _log_add(log_a0, log_s0)\n",
        "            log_a1 = _log_add(log_a1, log_s1)\n",
        "        else:\n",
        "            log_a0 = _log_sub(log_a0, log_s0)\n",
        "            log_a1 = _log_sub(log_a1, log_s1)\n",
        "\n",
        "        i += 1\n",
        "        if max(log_s0, log_s1) < -30:\n",
        "            break\n",
        "\n",
        "    return _log_add(log_a0, log_a1)\n",
        "\n",
        "\n",
        "def _compute_log_a(q, sigma, alpha):\n",
        "    \"\"\"Compute log(A_alpha) for any positive finite alpha.\"\"\"\n",
        "    if float(alpha).is_integer():\n",
        "        return _compute_log_a_int(q, sigma, int(alpha))\n",
        "    else:\n",
        "        return _compute_log_a_frac(q, sigma, alpha)\n",
        "\n",
        "\n",
        "def _log_erfc(x):\n",
        "    \"\"\"Compute log(erfc(x)) with high accuracy for large x.\"\"\"\n",
        "    try:\n",
        "        return math.log(2) + special.log_ndtr(-x * 2**.5)\n",
        "    except NameError:\n",
        "        # If log_ndtr is not available, approximate as follows:\n",
        "        r = special.erfc(x)\n",
        "        if r == 0.0:\n",
        "            # Using the Laurent series at infinity for the tail of the erfc function:\n",
        "            #     erfc(x) ~ exp(-x^2-.5/x^2+.625/x^4)/(x*pi^.5)\n",
        "            # To verify in Mathematica:\n",
        "            #     Series[Log[Erfc[x]] + Log[x] + Log[Pi]/2 + x^2, {x, Infinity, 6}]\n",
        "            return (-math.log(math.pi) / 2 - math.log(x) - x**2 - .5 * x**-2 +\n",
        "                    .625 * x**-4 - 37. / 24. * x**-6 + 353. / 64. * x**-8)\n",
        "        else:\n",
        "            return math.log(r)\n",
        "\n",
        "\n",
        "def _compute_delta(orders, rdp, eps):\n",
        "    \"\"\"Compute delta given a list of RDP values and target epsilon.\n",
        "\n",
        "  Args:\n",
        "    orders: An array (or a scalar) of orders.\n",
        "    rdp: A list (or a scalar) of RDP guarantees.\n",
        "    eps: The target epsilon.\n",
        "\n",
        "  Returns:\n",
        "    Pair of (delta, optimal_order).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If input is malformed.\n",
        "\n",
        "  \"\"\"\n",
        "    orders_vec = np.atleast_1d(orders)\n",
        "    rdp_vec = np.atleast_1d(rdp)\n",
        "\n",
        "    if len(orders_vec) != len(rdp_vec):\n",
        "        raise ValueError(\"Input lists must have the same length.\")\n",
        "\n",
        "    deltas = np.exp((rdp_vec - eps) * (orders_vec - 1))\n",
        "    idx_opt = np.argmin(deltas)\n",
        "    return min(deltas[idx_opt], 1.), orders_vec[idx_opt]\n",
        "\n",
        "\n",
        "def _compute_eps(orders, rdp, delta):\n",
        "    \"\"\"Compute epsilon given a list of RDP values and target delta.\n",
        "\n",
        "  Args:\n",
        "    orders: An array (or a scalar) of orders.\n",
        "    rdp: A list (or a scalar) of RDP guarantees.\n",
        "    delta: The target delta.\n",
        "\n",
        "  Returns:\n",
        "    Pair of (eps, optimal_order).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If input is malformed.\n",
        "\n",
        "  \"\"\"\n",
        "    orders_vec = np.atleast_1d(orders)\n",
        "    rdp_vec = np.atleast_1d(rdp)\n",
        "\n",
        "    if len(orders_vec) != len(rdp_vec):\n",
        "        raise ValueError(\"Input lists must have the same length.\")\n",
        "\n",
        "    eps = rdp_vec - math.log(delta) / (orders_vec - 1)\n",
        "\n",
        "    idx_opt = np.nanargmin(eps)  # Ignore NaNs\n",
        "    return eps[idx_opt], orders_vec[idx_opt]\n",
        "\n",
        "\n",
        "def _compute_rdp(q, sigma, alpha):\n",
        "    \"\"\"Compute RDP of the Sampled Gaussian mechanism at order alpha.\n",
        "\n",
        "  Args:\n",
        "    q: The sampling rate.\n",
        "    sigma: The std of the additive Gaussian noise.\n",
        "    alpha: The order at which RDP is computed.\n",
        "\n",
        "  Returns:\n",
        "    RDP at alpha, can be np.inf.\n",
        "  \"\"\"\n",
        "    if q == 0:\n",
        "        return 0\n",
        "\n",
        "    if q == 1.:\n",
        "        return alpha / (2 * sigma**2)\n",
        "\n",
        "    if np.isinf(alpha):\n",
        "        return np.inf\n",
        "\n",
        "    return _compute_log_a(q, sigma, alpha) / (alpha - 1)\n",
        "\n",
        "\n",
        "def compute_rdp(q, noise_multiplier, steps, orders):\n",
        "    \"\"\"Compute RDP of the Sampled Gaussian Mechanism.\n",
        "\n",
        "  Args:\n",
        "    q: The sampling rate.\n",
        "    noise_multiplier: The ratio of the standard deviation of the Gaussian noise\n",
        "        to the l2-sensitivity of the function to which it is added.\n",
        "    steps: The number of steps.\n",
        "    orders: An array (or a scalar) of RDP orders.\n",
        "\n",
        "  Returns:\n",
        "    The RDPs at all orders, can be np.inf.\n",
        "  \"\"\"\n",
        "    if np.isscalar(orders):\n",
        "        rdp = _compute_rdp(q, noise_multiplier, orders)\n",
        "    else:\n",
        "        rdp = np.array(\n",
        "            [_compute_rdp(q, noise_multiplier, order) for order in orders])\n",
        "\n",
        "    return rdp * steps\n",
        "\n",
        "\n",
        "def get_privacy_spent(orders, rdp, target_eps=None, target_delta=None):\n",
        "    \"\"\"Compute delta (or eps) for given eps (or delta) from RDP values.\n",
        "\n",
        "  Args:\n",
        "    orders: An array (or a scalar) of RDP orders.\n",
        "    rdp: An array of RDP values. Must be of the same length as the orders list.\n",
        "    target_eps: If not None, the epsilon for which we compute the corresponding\n",
        "              delta.\n",
        "    target_delta: If not None, the delta for which we compute the corresponding\n",
        "              epsilon. Exactly one of target_eps and target_delta must be None.\n",
        "\n",
        "  Returns:\n",
        "    eps, delta, opt_order.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If target_eps and target_delta are messed up.\n",
        "  \"\"\"\n",
        "    if target_eps is None and target_delta is None:\n",
        "        raise ValueError(\n",
        "            \"Exactly one out of eps and delta must be None. (Both are).\")\n",
        "\n",
        "    if target_eps is not None and target_delta is not None:\n",
        "        raise ValueError(\n",
        "            \"Exactly one out of eps and delta must be None. (None is).\")\n",
        "\n",
        "    if target_eps is not None:\n",
        "        delta, opt_order = _compute_delta(orders, rdp, target_eps)\n",
        "        return target_eps, delta, opt_order\n",
        "    else:\n",
        "        eps, opt_order = _compute_eps(orders, rdp, target_delta)\n",
        "        return eps, target_delta, opt_order\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRV9EuoRW-4h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## dp query\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "X0IYyH4pW9Jy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018, The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"An interface for differentially private query mechanisms.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "\n",
        "\n",
        "class DPQuery(object):\n",
        "  \"\"\"Interface for differentially private query mechanisms.\"\"\"\n",
        "\n",
        "  __metaclass__ = abc.ABCMeta\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def initial_global_state(self):\n",
        "    \"\"\"Returns the initial global state for the DPQuery.\"\"\"\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def derive_sample_params(self, global_state):\n",
        "    \"\"\"Given the global state, derives parameters to use for the next sample.\n",
        "\n",
        "    Args:\n",
        "      global_state: The current global state.\n",
        "\n",
        "    Returns:\n",
        "      Parameters to use to process records in the next sample.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def initial_sample_state(self, global_state, tensors):\n",
        "    \"\"\"Returns an initial state to use for the next sample.\n",
        "\n",
        "    Args:\n",
        "      global_state: The current global state.\n",
        "      tensors: A structure of tensors used as a template to create the initial\n",
        "        sample state.\n",
        "\n",
        "    Returns: An initial sample state.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def accumulate_record(self, params, sample_state, record):\n",
        "    \"\"\"Accumulates a single record into the sample state.\n",
        "\n",
        "    Args:\n",
        "      params: The parameters for the sample.\n",
        "      sample_state: The current sample state.\n",
        "      record: The record to accumulate.\n",
        "\n",
        "    Returns:\n",
        "      The updated sample state.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def get_noised_result(self, sample_state, global_state):\n",
        "    \"\"\"Gets query result after all records of sample have been accumulated.\n",
        "\n",
        "    Args:\n",
        "      sample_state: The sample state after all records have been accumulated.\n",
        "      global_state: The global state.\n",
        "\n",
        "    Returns:\n",
        "      A tuple (result, new_global_state) where \"result\" is the result of the\n",
        "      query and \"new_global_state\" is the updated global state.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h9a2OczBWzr5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## gausian query"
      ]
    },
    {
      "metadata": {
        "id": "8prVjW-GWwf-",
        "colab_type": "code",
        "outputId": "17418b56-e7c9-4cc1-9ff5-637b782bc536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018, The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Implements DPQuery interface for Gaussian average queries.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "nest = tf.contrib.framework.nest\n",
        "\n",
        "\n",
        "class GaussianSumQuery(DPQuery):\n",
        "  \"\"\"Implements DPQuery interface for Gaussian sum queries.\n",
        "\n",
        "  Accumulates clipped vectors, then adds Gaussian noise to the sum.\n",
        "  \"\"\"\n",
        "\n",
        "  # pylint: disable=invalid-name\n",
        "  _GlobalState = collections.namedtuple(\n",
        "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
        "\n",
        "  def __init__(self, l2_norm_clip, stddev):\n",
        "    \"\"\"Initializes the GaussianSumQuery.\n",
        "\n",
        "    Args:\n",
        "      l2_norm_clip: The clipping norm to apply to the global norm of each\n",
        "        record.\n",
        "      stddev: The stddev of the noise added to the sum.\n",
        "    \"\"\"\n",
        "    self._l2_norm_clip = l2_norm_clip\n",
        "    self._stddev = stddev\n",
        "\n",
        "  def initial_global_state(self):\n",
        "    \"\"\"Returns the initial global state for the GaussianSumQuery.\"\"\"\n",
        "    return self._GlobalState(float(self._l2_norm_clip), float(self._stddev))\n",
        "\n",
        "  def derive_sample_params(self, global_state):\n",
        "    \"\"\"Given the global state, derives parameters to use for the next sample.\n",
        "\n",
        "    Args:\n",
        "      global_state: The current global state.\n",
        "\n",
        "    Returns:\n",
        "      Parameters to use to process records in the next sample.\n",
        "    \"\"\"\n",
        "    return global_state.l2_norm_clip\n",
        "\n",
        "  def initial_sample_state(self, global_state, tensors):\n",
        "    \"\"\"Returns an initial state to use for the next sample.\n",
        "\n",
        "    Args:\n",
        "      global_state: The current global state.\n",
        "      tensors: A structure of tensors used as a template to create the initial\n",
        "        sample state.\n",
        "\n",
        "    Returns: An initial sample state.\n",
        "    \"\"\"\n",
        "    del global_state  # unused.\n",
        "    return nest.map_structure(tf.zeros_like, tensors)\n",
        "\n",
        "  def accumulate_record(self, params, sample_state, record):\n",
        "    \"\"\"Accumulates a single record into the sample state.\n",
        "\n",
        "    Args:\n",
        "      params: The parameters for the sample.\n",
        "      sample_state: The current sample state.\n",
        "      record: The record to accumulate.\n",
        "\n",
        "    Returns:\n",
        "      The updated sample state.\n",
        "    \"\"\"\n",
        "    l2_norm_clip = params\n",
        "    record_as_list = nest.flatten(record)\n",
        "    clipped_as_list, _ = tf.clip_by_global_norm(record_as_list, l2_norm_clip)\n",
        "    clipped = nest.pack_sequence_as(record, clipped_as_list)\n",
        "\n",
        "    return nest.map_structure(tf.add, sample_state, clipped)\n",
        "\n",
        "  def get_noised_result(self, sample_state, global_state, add_noise=True):\n",
        "    \"\"\"Gets noised sum after all records of sample have been accumulated.\n",
        "\n",
        "    Args:\n",
        "      sample_state: The sample state after all records have been accumulated.\n",
        "      global_state: The global state.\n",
        "\n",
        "    Returns:\n",
        "      A tuple (estimate, new_global_state) where \"estimate\" is the estimated\n",
        "      sum of the records and \"new_global_state\" is the updated global state.\n",
        "    \"\"\"\n",
        "    def add_noise(v):\n",
        "      if add_noise:\n",
        "        return v + tf.random_normal(tf.shape(v), stddev=global_state.stddev)\n",
        "      else:\n",
        "        return v\n",
        "\n",
        "\n",
        "    return nest.map_structure(add_noise, sample_state), global_state\n",
        "\n",
        "\n",
        "class GaussianAverageQuery(DPQuery):\n",
        "  \"\"\"Implements DPQuery interface for Gaussian average queries.\n",
        "\n",
        "  Accumulates clipped vectors, adds Gaussian noise, and normalizes.\n",
        "\n",
        "  Note that we use \"fixed-denominator\" estimation: the denominator should be\n",
        "  specified as the expected number of records per sample. Accumulating the\n",
        "  denominator separately would also be possible but would be produce a higher\n",
        "  variance estimator.\n",
        "  \"\"\"\n",
        "\n",
        "  # pylint: disable=invalid-name\n",
        "  _GlobalState = collections.namedtuple(\n",
        "      '_GlobalState', ['sum_state', 'denominator'])\n",
        "\n",
        "  def __init__(self, l2_norm_clip, sum_stddev, denominator):\n",
        "    \"\"\"Initializes the GaussianAverageQuery.\n",
        "\n",
        "    Args:\n",
        "      l2_norm_clip: The clipping norm to apply to the global norm of each\n",
        "        record.\n",
        "      sum_stddev: The stddev of the noise added to the sum (before\n",
        "        normalization).\n",
        "      denominator: The normalization constant (applied after noise is added to\n",
        "        the sum).\n",
        "    \"\"\"\n",
        "    self._numerator = GaussianSumQuery(l2_norm_clip, sum_stddev)\n",
        "    self._denominator = denominator\n",
        "\n",
        "  def initial_global_state(self):\n",
        "    \"\"\"Returns the initial global state for the GaussianAverageQuery.\"\"\"\n",
        "    sum_global_state = self._numerator.initial_global_state()\n",
        "    return self._GlobalState(sum_global_state, float(self._denominator))\n",
        "\n",
        "  def derive_sample_params(self, global_state):\n",
        "    \"\"\"Given the global state, derives parameters to use for the next sample.\n",
        "\n",
        "    Args:\n",
        "      global_state: The current global state.\n",
        "\n",
        "    Returns:\n",
        "      Parameters to use to process records in the next sample.\n",
        "    \"\"\"\n",
        "    return self._numerator.derive_sample_params(global_state.sum_state)\n",
        "\n",
        "  def initial_sample_state(self, global_state, tensors):\n",
        "    \"\"\"Returns an initial state to use for the next sample.\n",
        "\n",
        "    Args:\n",
        "      global_state: The current global state.\n",
        "      tensors: A structure of tensors used as a template to create the initial\n",
        "        sample state.\n",
        "\n",
        "    Returns: An initial sample state.\n",
        "    \"\"\"\n",
        "    # GaussianAverageQuery has no state beyond the sum state.\n",
        "    return self._numerator.initial_sample_state(global_state.sum_state, tensors)\n",
        "\n",
        "  def accumulate_record(self, params, sample_state, record):\n",
        "    \"\"\"Accumulates a single record into the sample state.\n",
        "\n",
        "    Args:\n",
        "      params: The parameters for the sample.\n",
        "      sample_state: The current sample state.\n",
        "      record: The record to accumulate.\n",
        "\n",
        "    Returns:\n",
        "      The updated sample state.\n",
        "    \"\"\"\n",
        "    \n",
        "    return self._numerator.accumulate_record(params, sample_state, record)\n",
        "\n",
        "  def get_noised_result(self, sample_state, global_state, add_noise=True):\n",
        "    \"\"\"Gets noised average after all records of sample have been accumulated.\n",
        "\n",
        "    Args:\n",
        "      sample_state: The sample state after all records have been accumulated.\n",
        "      global_state: The global state.\n",
        "\n",
        "    Returns:\n",
        "      A tuple (estimate, new_global_state) where \"estimate\" is the estimated\n",
        "      average of the records and \"new_global_state\" is the updated global state.\n",
        "    \"\"\"\n",
        "    noised_sum, new_sum_global_state = self._numerator.get_noised_result(\n",
        "        sample_state, global_state.sum_state, add_noise)\n",
        "    new_global_state = self._GlobalState(\n",
        "        new_sum_global_state, global_state.denominator)\n",
        "    def normalize(v):\n",
        "      return tf.truediv(v, global_state.denominator)\n",
        "\n",
        "    return nest.map_structure(normalize, noised_sum), new_global_state\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g3iUDbgoUl4G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## our_dp_optimizer"
      ]
    },
    {
      "metadata": {
        "id": "UzHTI--TRyU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018, The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Differentially private optimizers for TensorFlow.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def make_optimizer_class(cls):\n",
        "    \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
        "    if (tf.train.Optimizer.compute_gradients.__code__ is\n",
        "            not cls.compute_gradients.__code__):\n",
        "        tf.logging.warning(\n",
        "            'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
        "            'method compute_gradients(). Check to ensure that '\n",
        "            'make_optimizer_class() does not interfere with overridden version.',\n",
        "            cls.__name__)\n",
        "\n",
        "    class DPOptimizerClass(cls):\n",
        "        \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
        "\n",
        "        def __init__(\n",
        "                self,\n",
        "                l2_norm_clip,\n",
        "                noise_multiplier,\n",
        "                dp_average_query,\n",
        "                num_microbatches,\n",
        "                unroll_microbatches=False,\n",
        "                *args,  # pylint: disable=keyword-arg-before-vararg\n",
        "                **kwargs):\n",
        "            super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
        "            self._dp_average_query = dp_average_query\n",
        "            self._num_microbatches = num_microbatches\n",
        "            self._global_state = self._dp_average_query.initial_global_state()\n",
        "\n",
        "            # TODO(b/122613513): Set unroll_microbatches=True to avoid this bug.\n",
        "            # Beware: When num_microbatches is large (>100), enabling this parameter\n",
        "            # may cause an OOM error.\n",
        "            self._unroll_microbatches = unroll_microbatches\n",
        "\n",
        "        def dp_compute_gradients(self,\n",
        "                                 loss,\n",
        "                                 var_list,\n",
        "                                 gate_gradients=tf.train.Optimizer.GATE_OP,\n",
        "                                 aggregation_method=None,\n",
        "                                 colocate_gradients_with_ops=False,\n",
        "                                 grad_loss=None,\n",
        "                                 add_noise=True):\n",
        "\n",
        "            # Note: it would be closer to the correct i.i.d. sampling of records if\n",
        "            # we sampled each microbatch from the appropriate binomial distribution,\n",
        "            # although that still wouldn't be quite correct because it would be\n",
        "            # sampling from the dataset without replacement.\n",
        "            microbatches_losses = tf.reshape(loss,\n",
        "                                             [self._num_microbatches, -1])\n",
        "            sample_params = (self._dp_average_query.derive_sample_params(\n",
        "                self._global_state))\n",
        "\n",
        "            def process_microbatch(i, sample_state):\n",
        "                \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
        "                grads, _ = zip(*super(cls, self).compute_gradients(\n",
        "                    tf.gather(microbatches_losses, [i]), var_list,\n",
        "                    gate_gradients, aggregation_method,\n",
        "                    colocate_gradients_with_ops, grad_loss))\n",
        "\n",
        "                # Converts tensor to list to replace None gradients with zero\n",
        "                grads1 = list(grads)\n",
        "\n",
        "                for inx in range(0, len(grads)):\n",
        "                    if (grads[inx] == None):\n",
        "                        grads1[inx] = tf.zeros_like(var_list[inx])\n",
        "\n",
        "                grads_list = grads1\n",
        "                sample_state = self._dp_average_query.accumulate_record(\n",
        "                    sample_params, sample_state, grads_list)\n",
        "\n",
        "                return sample_state\n",
        "\n",
        "            if var_list is None:\n",
        "                var_list = (tf.trainable_variables() + tf.get_collection(\n",
        "                    tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n",
        "            sample_state = self._dp_average_query.initial_sample_state(\n",
        "                self._global_state, var_list)\n",
        "\n",
        "            if self._unroll_microbatches:\n",
        "                for idx in range(self._num_microbatches):\n",
        "                    sample_state = process_microbatch(idx, sample_state)\n",
        "            else:\n",
        "                # Use of while_loop here requires that sample_state be a nested\n",
        "                # structure of tensors. In general, we would prefer to allow it to be\n",
        "                # an arbitrary opaque type.\n",
        "\n",
        "                cond_fn = lambda i, _: tf.less(i, self._num_microbatches)\n",
        "                body_fn = lambda i, state: [\n",
        "                    tf.add(i, 1), process_microbatch(i, state)\n",
        "                ]\n",
        "\n",
        "                idx = tf.constant(0)\n",
        "                _, sample_state = tf.while_loop(cond_fn, body_fn,\n",
        "                                                [idx, sample_state])\n",
        "\n",
        "            final_grads, self._global_state = (\n",
        "                self._dp_average_query.get_noised_result(\n",
        "                    sample_state, self._global_state, add_noise))\n",
        "\n",
        "            return (final_grads)\n",
        "\n",
        "        def minimize(self,\n",
        "                          d_loss_real,\n",
        "                          d_loss_fake,\n",
        "                          global_step=None,\n",
        "                          var_list=None,\n",
        "                          gate_gradients=tf.train.Optimizer.GATE_OP,\n",
        "                          aggregation_method=None,\n",
        "                          colocate_gradients_with_ops=False,\n",
        "                          name=None,\n",
        "                          grad_loss=None):\n",
        "            \"\"\"Minimize using sanitized gradients\n",
        "\n",
        "            Args:\n",
        "              d_loss_real: the loss tensor for real data\n",
        "              d_loss_fake: the loss tensor for fake data\n",
        "              global_step: the optional global step.\n",
        "              var_list: the optional variables.\n",
        "              name: the optional name.\n",
        "            Returns:\n",
        "              the operation that runs one step of DP gradient descent.\n",
        "            \"\"\"\n",
        "\n",
        "            # First validate the var_list\n",
        "\n",
        "            if var_list is None:\n",
        "                var_list = tf.trainable_variables()\n",
        "            for var in var_list:\n",
        "                if not isinstance(var, tf.Variable):\n",
        "                    raise TypeError(\"Argument is not a variable.Variable: %s\" %\n",
        "                                    var)\n",
        "\n",
        "            # ------------------  OUR METHOD --------------------------------\n",
        "\n",
        "            r_grads = self.dp_compute_gradients(\n",
        "                d_loss_real,\n",
        "                var_list=var_list,\n",
        "                gate_gradients=gate_gradients,\n",
        "                aggregation_method=aggregation_method,\n",
        "                colocate_gradients_with_ops=colocate_gradients_with_ops,\n",
        "                grad_loss=grad_loss, add_noise = True)\n",
        "\n",
        "            f_grads = self.dp_compute_gradients(\n",
        "                d_loss_fake,\n",
        "                var_list=var_list,\n",
        "                gate_gradients=gate_gradients,\n",
        "                aggregation_method=aggregation_method,\n",
        "                colocate_gradients_with_ops=colocate_gradients_with_ops,\n",
        "                grad_loss=grad_loss,\n",
        "                add_noise=False)\n",
        "\n",
        "            # Compute the overall gradients \n",
        "            s_grads = [(r_grads[idx] + f_grads[idx])\n",
        "                       for idx in range(len(r_grads))]\n",
        "\n",
        "            sanitized_grads_and_vars = list(zip(s_grads, var_list))\n",
        "            self._assert_valid_dtypes(\n",
        "                [v for g, v in sanitized_grads_and_vars if g is not None])\n",
        "\n",
        "            # Apply the overall gradients\n",
        "            apply_grads = self.apply_gradients(sanitized_grads_and_vars,\n",
        "                                               global_step=global_step,\n",
        "                                               name=name)\n",
        "\n",
        "            return apply_grads\n",
        "\n",
        "         # -----------------------------------------------------------------\n",
        "\n",
        "    return DPOptimizerClass\n",
        "\n",
        "\n",
        "def make_gaussian_optimizer_class(cls):\n",
        "    \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
        "\n",
        "    class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
        "        \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
        "\n",
        "        def __init__(\n",
        "                self,\n",
        "                l2_norm_clip,\n",
        "                noise_multiplier,\n",
        "                num_microbatches,\n",
        "                unroll_microbatches=False,\n",
        "                *args,  # pylint: disable=keyword-arg-before-vararg\n",
        "                **kwargs):\n",
        "            dp_average_query = GaussianAverageQuery(\n",
        "                l2_norm_clip, l2_norm_clip * noise_multiplier,\n",
        "                num_microbatches)\n",
        "            self.l2_norm_clip = l2_norm_clip\n",
        "            self.noise_multiplier = noise_multiplier\n",
        "\n",
        "            super(DPGaussianOptimizerClass,\n",
        "                  self).__init__(l2_norm_clip, noise_multiplier,\n",
        "                                 dp_average_query, num_microbatches,\n",
        "                                 unroll_microbatches, *args, **kwargs)\n",
        "\n",
        "    return DPGaussianOptimizerClass\n",
        "\n",
        "\n",
        "DPAdagradOptimizer = make_optimizer_class(tf.train.AdagradOptimizer)\n",
        "DPAdamOptimizer = make_optimizer_class(tf.train.AdamOptimizer)\n",
        "DPGradientDescentOptimizer = make_optimizer_class(\n",
        "    tf.train.GradientDescentOptimizer)\n",
        "\n",
        "DPAdagradGaussianOptimizer = make_gaussian_optimizer_class(\n",
        "    tf.train.AdagradOptimizer)\n",
        "DPAdamGaussianOptimizer = make_gaussian_optimizer_class(tf.train.AdamOptimizer)\n",
        "DPGradientDescentGaussianOptimizer = make_gaussian_optimizer_class(\n",
        "    tf.train.GradientDescentOptimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4uknVSySjrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## gan.ops"
      ]
    },
    {
      "metadata": {
        "id": "XxtMZQYgO3TB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Most codes from https://github.com/carpedm20/DCGAN-tensorflow\n",
        "\"\"\"\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "if \"concat_v2\" in dir(tf):\n",
        "\n",
        "    def concat(tensors, axis, *args, **kwargs):\n",
        "        return tf.concat_v2(tensors, axis, *args, **kwargs)\n",
        "else:\n",
        "\n",
        "    def concat(tensors, axis, *args, **kwargs):\n",
        "        return tf.concat(tensors, axis, *args, **kwargs)\n",
        "\n",
        "\n",
        "def bn(x, is_training, scope):\n",
        "    return tf.contrib.layers.batch_norm(x,\n",
        "                                        decay=0.9,\n",
        "                                        updates_collections=None,\n",
        "                                        epsilon=1e-5,\n",
        "                                        scale=True,\n",
        "                                        is_training=is_training,\n",
        "                                        scope=scope)\n",
        "\n",
        "\n",
        "def conv_out_size_same(size, stride):\n",
        "    return int(math.ceil(float(size) / float(stride)))\n",
        "\n",
        "\n",
        "def conv_cond_concat(x, y):\n",
        "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
        "    x_shapes = x.get_shape()\n",
        "    y_shapes = y.get_shape()\n",
        "    return concat(\n",
        "        [x, y * tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])],\n",
        "        3)\n",
        "\n",
        "\n",
        "def conv2d(input_,\n",
        "           output_dim,\n",
        "           k_h=5,\n",
        "           k_w=5,\n",
        "           d_h=2,\n",
        "           d_w=2,\n",
        "           stddev=0.02,\n",
        "           name=\"conv2d\"):\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable(\n",
        "            'w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
        "            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
        "        conv = tf.nn.conv2d(input_,\n",
        "                            w,\n",
        "                            strides=[1, d_h, d_w, 1],\n",
        "                            padding='SAME')\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_dim],\n",
        "                                 initializer=tf.constant_initializer(0.0))\n",
        "        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
        "\n",
        "        return conv\n",
        "\n",
        "\n",
        "def deconv2d(input_,\n",
        "             output_shape,\n",
        "             k_h=5,\n",
        "             k_w=5,\n",
        "             d_h=2,\n",
        "             d_w=2,\n",
        "             name=\"deconv2d\",\n",
        "             stddev=0.02,\n",
        "             with_w=False):\n",
        "    with tf.variable_scope(name):\n",
        "        # filter : [height, width, output_channels, in_channels]\n",
        "        w = tf.get_variable(\n",
        "            'w', [k_h, k_w, output_shape[-1],\n",
        "                  input_.get_shape()[-1]],\n",
        "            initializer=tf.random_normal_initializer(stddev=stddev))\n",
        "\n",
        "        try:\n",
        "            deconv = tf.nn.conv2d_transpose(input_,\n",
        "                                            w,\n",
        "                                            output_shape=output_shape,\n",
        "                                            strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        # Support for verisons of TensorFlow before 0.7.0\n",
        "        except AttributeError:\n",
        "            deconv = tf.nn.deconv2d(input_,\n",
        "                                    w,\n",
        "                                    output_shape=output_shape,\n",
        "                                    strides=[1, d_h, d_w, 1])\n",
        "\n",
        "        biases = tf.get_variable('biases', [output_shape[-1]],\n",
        "                                 initializer=tf.constant_initializer(0.0))\n",
        "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
        "\n",
        "        if with_w:\n",
        "            return deconv, w, biases\n",
        "        else:\n",
        "            return deconv\n",
        "\n",
        "\n",
        "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
        "    return tf.maximum(x, leak * x)\n",
        "\n",
        "\n",
        "def linear(input_,\n",
        "           output_size,\n",
        "           scope=None,\n",
        "           stddev=0.02,\n",
        "           bias_start=0.0,\n",
        "           with_w=False):\n",
        "    shape = input_.get_shape().as_list()\n",
        "\n",
        "    with tf.variable_scope(scope or \"Linear\"):\n",
        "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
        "                                 tf.random_normal_initializer(stddev=stddev))\n",
        "        bias = tf.get_variable(\"bias\", [output_size],\n",
        "                               initializer=tf.constant_initializer(bias_start))\n",
        "        if with_w:\n",
        "            return tf.matmul(input_, matrix) + bias, matrix, bias\n",
        "        else:\n",
        "            return tf.matmul(input_, matrix) + bias\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fah9gybVSXUU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## OUR DP CGAN"
      ]
    },
    {
      "metadata": {
        "id": "ArOO5od4InHL",
        "colab_type": "code",
        "outputId": "ffcf8e99-18dc-4637-92ad-8915e1ff78c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import division\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "class OUR_DP_CGAN(object):\n",
        "    model_name = \"OUR_DP_CGAN\"  # name for checkpoint\n",
        "\n",
        "    def __init__(self, sess, epoch, batch_size, z_dim, epsilon, delta, sigma,\n",
        "                 clip_value, lr, dataset_name, base_dir, checkpoint_dir,\n",
        "                 result_dir, log_dir):\n",
        "        self.sess = sess\n",
        "        self.dataset_name = dataset_name\n",
        "        self.base_dir = base_dir\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.log_dir = log_dir\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.noise_multiplier = sigma\n",
        "        self.l2_norm_clip = clip_value\n",
        "        self.lr = lr\n",
        "\n",
        "        if dataset_name == 'mnist' or dataset_name == 'fashion-mnist':\n",
        "            # parameters\n",
        "            self.input_height = 28\n",
        "            self.input_width = 28\n",
        "            self.output_height = 28\n",
        "            self.output_width = 28\n",
        "\n",
        "            self.z_dim = z_dim  # dimension of noise-vector\n",
        "            self.y_dim = 10  # dimension of condition-vector (label)\n",
        "            self.c_dim = 1\n",
        "\n",
        "            # train\n",
        "            self.learningRateD = self.lr\n",
        "            self.learningRateG = self.learningRateD * 5\n",
        "            self.beta1 = 0.5\n",
        "            self.beta2 = 0.99\n",
        "            # test\n",
        "            self.sample_num = 64  # number of generated images to be saved\n",
        "\n",
        "            # load mnist\n",
        "            self.data_X, self.data_y = load_mnist(train = True)\n",
        "\n",
        "            # get number of batches for a single epoch\n",
        "            self.num_batches = len(self.data_X) // self.batch_size\n",
        "\n",
        "        elif dataset_name == 'cifar10':\n",
        "            # parameters\n",
        "            self.input_height = 32\n",
        "            self.input_width = 32\n",
        "            self.output_height = 32\n",
        "            self.output_width = 32\n",
        "\n",
        "            self.z_dim = 100  # dimension of noise-vector\n",
        "            self.y_dim = 10  # dimension of condition-vector (label)\n",
        "            self.c_dim = 3  # color dimension\n",
        "\n",
        "            # train\n",
        "            # self.learning_rate = 0.0002 # 1e-3, 1e-4\n",
        "            self.learningRateD = 1e-3\n",
        "            self.learningRateG = 1e-4\n",
        "            self.beta1 = 0.5\n",
        "            self.beta2 = 0.99\n",
        "\n",
        "            # test\n",
        "            self.sample_num = 64  # number of generated images to be saved\n",
        "\n",
        "            # load cifar10\n",
        "            self.data_X, self.data_y = load_cifar10(train=True)\n",
        "\n",
        "            self.num_batches = len(self.data_X) // self.batch_size\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def discriminator(self, x, y, is_training=True, reuse=False):\n",
        "        # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
        "        # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
        "        with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
        "\n",
        "            # merge image and label\n",
        "            if (self.dataset_name == \"mnist\"):\n",
        "                y = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n",
        "                x = conv_cond_concat(x, y)\n",
        "\n",
        "                net = lrelu(conv2d(x, 64, 4, 4, 2, 2, name='d_conv1'))\n",
        "                net = lrelu(\n",
        "                    bn(conv2d(net, 128, 4, 4, 2, 2, name='d_conv2'),\n",
        "                       is_training=is_training,\n",
        "                       scope='d_bn2'))\n",
        "                net = tf.reshape(net, [self.batch_size, -1])\n",
        "                net = lrelu(\n",
        "                    bn(linear(net, 1024, scope='d_fc3'),\n",
        "                       is_training=is_training,\n",
        "                       scope='d_bn3'))\n",
        "                out_logit = linear(net, 1, scope='d_fc4')\n",
        "                out = tf.nn.sigmoid(out_logit)\n",
        "\n",
        "            elif (self.dataset_name == \"cifar10\"):\n",
        "\n",
        "                y = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n",
        "                x = conv_cond_concat(x, y)\n",
        "                lrelu_slope = 0.2\n",
        "                kernel_size = 5\n",
        "                w_init = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "                net = lrelu(\n",
        "                    conv2d(x,\n",
        "                           64,\n",
        "                           5,\n",
        "                           5,\n",
        "                           2,\n",
        "                           2,\n",
        "                           name='d_conv1' + '_' + self.dataset_name))\n",
        "               \n",
        "                net = lrelu(\n",
        "                    bn(conv2d(net,\n",
        "                              128,\n",
        "                              5,\n",
        "                              5,\n",
        "                              2,\n",
        "                              2,\n",
        "                              name='d_conv2' + '_' + self.dataset_name),\n",
        "                       is_training=is_training,\n",
        "                       scope='d_bn2'))\n",
        "         \n",
        "                net = lrelu(\n",
        "                    bn(conv2d(net,\n",
        "                              256,\n",
        "                              5,\n",
        "                              5,\n",
        "                              2,\n",
        "                              2,\n",
        "                              name='d_conv3' + '_' + self.dataset_name),\n",
        "                       is_training=is_training,\n",
        "                       scope='d_bn3'))\n",
        "                \n",
        "                net = lrelu(\n",
        "                    bn(conv2d(net,\n",
        "                              512,\n",
        "                              5,\n",
        "                              5,\n",
        "                              2,\n",
        "                              2,\n",
        "                              name='d_conv4' + '_' + self.dataset_name),\n",
        "                       is_training=is_training,\n",
        "                       scope='d_bn4'))\n",
        "               \n",
        "                net = tf.reshape(net, [self.batch_size, -1])\n",
        "               \n",
        "                out_logit = linear(net,\n",
        "                                   1,\n",
        "                                   scope='d_fc5' + '_' + self.dataset_name)\n",
        "                \n",
        "                out = tf.nn.sigmoid(out_logit)\n",
        "\n",
        "            return out, out_logit\n",
        "\n",
        "    def generator(self, z, y, is_training=True, reuse=False):\n",
        "        # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
        "        # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
        "        with tf.variable_scope(\"generator\", reuse=reuse):\n",
        "\n",
        "            if (self.dataset_name == \"mnist\"):\n",
        "                # merge noise and label\n",
        "                z = concat([z, y], 1)\n",
        "\n",
        "                net = tf.nn.relu(\n",
        "                    bn(linear(z, 1024, scope='g_fc1'),\n",
        "                       is_training=is_training,\n",
        "                       scope='g_bn1'))\n",
        "                net = tf.nn.relu(\n",
        "                    bn(linear(net, 128 * 7 * 7, scope='g_fc2'),\n",
        "                       is_training=is_training,\n",
        "                       scope='g_bn2'))\n",
        "                net = tf.reshape(net, [self.batch_size, 7, 7, 128])\n",
        "                net = tf.nn.relu(\n",
        "                    bn(deconv2d(net, [self.batch_size, 14, 14, 64],\n",
        "                                4,\n",
        "                                4,\n",
        "                                2,\n",
        "                                2,\n",
        "                                name='g_dc3'),\n",
        "                       is_training=is_training,\n",
        "                       scope='g_bn3'))\n",
        "\n",
        "                out = tf.nn.sigmoid(\n",
        "                    deconv2d(net, [self.batch_size, 28, 28, 1],\n",
        "                             4,\n",
        "                             4,\n",
        "                             2,\n",
        "                             2,\n",
        "                             name='g_dc4'))\n",
        "\n",
        "            elif (self.dataset_name == \"cifar10\"):\n",
        "                h_size = 32\n",
        "                h_size_2 = 16\n",
        "                h_size_4 = 8\n",
        "                h_size_8 = 4\n",
        "                h_size_16 = 2\n",
        "\n",
        "                z = concat([z, y], 1)\n",
        "\n",
        "                net = linear(z,\n",
        "                             512 * h_size_16 * h_size_16,\n",
        "                             scope='g_fc1' + '_' + self.dataset_name)\n",
        "                \n",
        "                net = tf.nn.relu(\n",
        "                    bn(tf.reshape(\n",
        "                        net, [self.batch_size, h_size_16, h_size_16, 512]),\n",
        "                       is_training=is_training,\n",
        "                       scope='g_bn1'))\n",
        "                \n",
        "                net = tf.nn.relu(\n",
        "                    bn(deconv2d(net,\n",
        "                                [self.batch_size, h_size_8, h_size_8, 256],\n",
        "                                5,\n",
        "                                5,\n",
        "                                2,\n",
        "                                2,\n",
        "                                name='g_dc2' + '_' + self.dataset_name),\n",
        "                       is_training=is_training,\n",
        "                       scope='g_bn2'))\n",
        "               \n",
        "                net = tf.nn.relu(\n",
        "                    bn(deconv2d(net,\n",
        "                                [self.batch_size, h_size_4, h_size_4, 128],\n",
        "                                5,\n",
        "                                5,\n",
        "                                2,\n",
        "                                2,\n",
        "                                name='g_dc3' + '_' + self.dataset_name),\n",
        "                       is_training=is_training,\n",
        "                       scope='g_bn3'))\n",
        "                \n",
        "                net = tf.nn.relu(\n",
        "                    bn(deconv2d(net, [self.batch_size, h_size_2, h_size_2, 64],\n",
        "                                5,\n",
        "                                5,\n",
        "                                2,\n",
        "                                2,\n",
        "                                name='g_dc4' + '_' + self.dataset_name),\n",
        "                       is_training=is_training,\n",
        "                       scope='g_bn4'))\n",
        "                \n",
        "                out = tf.nn.tanh(\n",
        "                    deconv2d(net, [\n",
        "                        self.batch_size, self.output_height, self.output_width,\n",
        "                        self.c_dim\n",
        "                    ],\n",
        "                             5,\n",
        "                             5,\n",
        "                             2,\n",
        "                             2,\n",
        "                             name='g_dc5' + '_' + self.dataset_name))\n",
        "\n",
        "            return out\n",
        "\n",
        "    def build_model(self):\n",
        "        # some parameters\n",
        "        image_dims = [self.input_height, self.input_width, self.c_dim]\n",
        "        bs = self.batch_size\n",
        "        \n",
        "        \"\"\" Graph Input \"\"\"\n",
        "        # images\n",
        "        self.inputs = tf.placeholder(tf.float32, [bs] + image_dims,\n",
        "                                     name='real_images')\n",
        "\n",
        "        # labels\n",
        "        self.y = tf.placeholder(tf.float32, [bs, self.y_dim], name='y')\n",
        "\n",
        "        # noises\n",
        "        self.z = tf.placeholder(tf.float32, [bs, self.z_dim], name='z')\n",
        "        \"\"\" Loss Function \"\"\"\n",
        "\n",
        "        # output of D for real images\n",
        "        D_real, D_real_logits = self.discriminator(self.inputs,\n",
        "                                                   self.y,\n",
        "                                                   is_training=True,\n",
        "                                                   reuse=False)\n",
        "\n",
        "        # output of D for fake images\n",
        "        G = self.generator(self.z, self.y, is_training=True, reuse=False)\n",
        "        D_fake, D_fake_logits = self.discriminator(G,\n",
        "                                                   self.y,\n",
        "                                                   is_training=True,\n",
        "                                                   reuse=True)\n",
        "\n",
        "        # get loss for discriminator\n",
        "        d_loss_real = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
        "        d_loss_fake = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
        "\n",
        "        self.d_loss_real_vec = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=D_real_logits, labels=tf.ones_like(D_real))\n",
        "        self.d_loss_fake_vec = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits=D_fake_logits, labels=tf.zeros_like(D_fake))\n",
        "\n",
        "        self.d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        # get loss for generator\n",
        "        self.g_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n",
        "        \n",
        "        \"\"\" Training \"\"\"\n",
        "        # divide trainable variables into a group for D and a group for G\n",
        "        t_vars = tf.trainable_variables()\n",
        "        d_vars = [\n",
        "            var for var in t_vars if var.name.startswith('discriminator')\n",
        "        ]\n",
        "        g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
        "\n",
        "        # optimizers\n",
        "        with tf.control_dependencies(tf.get_collection(\n",
        "                tf.GraphKeys.UPDATE_OPS)):\n",
        "\n",
        "            d_optim_init = DPGradientDescentGaussianOptimizer(\n",
        "                l2_norm_clip=self.l2_norm_clip,\n",
        "                noise_multiplier=self.noise_multiplier,\n",
        "                num_microbatches=self.batch_size,\n",
        "                learning_rate=self.learningRateD)\n",
        "\n",
        "            global_step = tf.train.get_global_step()\n",
        "\n",
        "            self.d_optim = d_optim_init.minimize(\n",
        "                d_loss_real=self.d_loss_real_vec,\n",
        "                d_loss_fake=self.d_loss_fake_vec,\n",
        "                global_step=global_step,\n",
        "                var_list=d_vars)\n",
        "\n",
        "            optimizer = DPGradientDescentGaussianOptimizer(\n",
        "                l2_norm_clip=self.l2_norm_clip,\n",
        "                noise_multiplier=self.noise_multiplier,\n",
        "                num_microbatches=self.batch_size,\n",
        "                learning_rate=self.learningRateD)\n",
        "\n",
        "            self.g_optim = tf.train.GradientDescentOptimizer(self.learningRateG) \\\n",
        "                        .minimize(self.g_loss, var_list=g_vars)\n",
        "            \n",
        "        \"\"\"\" Testing \"\"\"\n",
        "        self.fake_images = self.generator(self.z,\n",
        "                                          self.y,\n",
        "                                          is_training=False,\n",
        "                                          reuse=True)\n",
        "        \"\"\" Summary \"\"\"\n",
        "        d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
        "        d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
        "        d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
        "        g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
        "\n",
        "        # final summary operations\n",
        "        self.g_sum = tf.summary.merge([d_loss_fake_sum, g_loss_sum])\n",
        "        self.d_sum = tf.summary.merge([d_loss_real_sum, d_loss_sum])\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # initialize all variables\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        # graph inputs for visualize training results\n",
        "        self.sample_z = np.random.uniform(-1,\n",
        "                                          1,\n",
        "                                          size=(self.batch_size, self.z_dim))\n",
        "        self.test_labels = self.data_y[0:self.batch_size]\n",
        "\n",
        "        # saver to save model\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # summary writer\n",
        "        self.writer = tf.summary.FileWriter(\n",
        "            self.log_dir + '/' + self.model_name, self.sess.graph)\n",
        "\n",
        "        # restore check-point if it exits\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        if could_load:\n",
        "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
        "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
        "            counter = checkpoint_counter\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            start_epoch = 0\n",
        "            start_batch_id = 0\n",
        "            counter = 1\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        # loop for epoch\n",
        "        epoch = start_epoch\n",
        "        should_terminate = False\n",
        "        \n",
        "        while (epoch < self.epoch and not should_terminate):\n",
        "\n",
        "            # get batch data\n",
        "            for idx in range(start_batch_id, self.num_batches):\n",
        "                batch_images = self.data_X[idx * self.batch_size:(idx + 1) *\n",
        "                                           self.batch_size]\n",
        "                batch_labels = self.data_y[idx * self.batch_size:(idx + 1) *\n",
        "                                           self.batch_size]\n",
        "                batch_z = np.random.uniform(\n",
        "                    -1, 1, [self.batch_size, self.z_dim]).astype(np.float32)\n",
        "\n",
        "                # update D network\n",
        "\n",
        "                _, summary_str, d_loss = self.sess.run(\n",
        "                    [self.d_optim, self.d_sum, self.d_loss],\n",
        "                    feed_dict={\n",
        "                        self.inputs: batch_images,\n",
        "                        self.y: batch_labels,\n",
        "                        self.z: batch_z\n",
        "                    })\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                eps = self.compute_epsilon((epoch * self.num_batches) + idx)\n",
        "\n",
        "                if (eps > self.epsilon):\n",
        "\n",
        "                    should_terminate = True\n",
        "                    print(\"TERMINATE !! Run out of Privacy Budget.....\")\n",
        "                    epoch = self.epoch\n",
        "                    break\n",
        "\n",
        "                # update G network\n",
        "                _, summary_str, g_loss = self.sess.run(\n",
        "                    [self.g_optim, self.g_sum, self.g_loss],\n",
        "                    feed_dict={\n",
        "                        self.inputs: batch_images,\n",
        "                        self.y: batch_labels,\n",
        "                        self.z: batch_z\n",
        "                    })\n",
        "\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # display training status\n",
        "                counter += 1\n",
        "                _ = self.sess.run(self.fake_images,\n",
        "                                  feed_dict={\n",
        "                                      self.z: self.sample_z,\n",
        "                                      self.y: self.test_labels\n",
        "                                  })\n",
        "\n",
        "                # save training results for every 100 steps\n",
        "                if np.mod(counter, 100) == 0:\n",
        "                    print(\"Iteration : \" + str(idx) + \" Eps: \" + str(eps))\n",
        "                    samples = self.sess.run(self.fake_images,\n",
        "                                            feed_dict={\n",
        "                                                self.z: self.sample_z,\n",
        "                                                self.y: self.test_labels\n",
        "                                            })\n",
        "                    tot_num_samples = min(self.sample_num, self.batch_size)\n",
        "                    manifold_h = int(np.floor(np.sqrt(tot_num_samples)))\n",
        "                    manifold_w = int(np.floor(np.sqrt(tot_num_samples)))\n",
        "                    save_images(\n",
        "                        samples[:manifold_h * manifold_w, :, :, :],\n",
        "                        [manifold_h, manifold_w],\n",
        "                        check_folder(self.result_dir + '/' + self.model_dir) +\n",
        "                        '/' + self.model_name +\n",
        "                        '_train_{:02d}_{:04d}.png'.format(epoch, idx))\n",
        "            epoch = epoch + 1\n",
        "\n",
        "            # After an epoch, start_batch_id is set to zero\n",
        "            # non-zero value is only for the first epoch after loading pre-trained model\n",
        "            start_batch_id = 0\n",
        "\n",
        "            # save model\n",
        "            self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "            # show temporal results\n",
        "            if (self.dataset_name == 'mnist'):\n",
        "                self.visualize_results_MNIST(epoch)\n",
        "            elif (self.dataset_name == 'cifar10'):\n",
        "                self.visualize_results_CIFAR(epoch)\n",
        "\n",
        "        # save model for final step\n",
        "        self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "\n",
        "        def compute_fpr_tpr_roc(Y_test, Y_score):\n",
        "            n_classes = Y_score.shape[1]\n",
        "            false_positive_rate = dict()\n",
        "            true_positive_rate = dict()\n",
        "            roc_auc = dict()\n",
        "            for class_cntr in range(n_classes):\n",
        "                false_positive_rate[class_cntr], true_positive_rate[\n",
        "                    class_cntr], _ = roc_curve(Y_test[:, class_cntr],\n",
        "                                               Y_score[:, class_cntr])\n",
        "                roc_auc[class_cntr] = auc(false_positive_rate[class_cntr],\n",
        "                                          true_positive_rate[class_cntr])\n",
        "\n",
        "            # Compute micro-average ROC curve and ROC area\n",
        "            false_positive_rate[\"micro\"], true_positive_rate[\n",
        "                \"micro\"], _ = roc_curve(Y_test.ravel(), Y_score.ravel())\n",
        "            roc_auc[\"micro\"] = auc(false_positive_rate[\"micro\"],\n",
        "                                   true_positive_rate[\"micro\"])\n",
        "\n",
        "            return false_positive_rate, true_positive_rate, roc_auc\n",
        "\n",
        "        def classify(X_train,\n",
        "                     Y_train,\n",
        "                     X_test,\n",
        "                     classiferName,\n",
        "                     random_state_value=0):\n",
        "            if classiferName == \"lr\":\n",
        "                classifier = OneVsRestClassifier(\n",
        "                    LogisticRegression(solver='lbfgs',\n",
        "                                       multi_class='multinomial',\n",
        "                                       random_state=random_state_value))\n",
        "            elif classiferName == \"mlp\":\n",
        "                classifier = OneVsRestClassifier(\n",
        "                    MLPClassifier(random_state=random_state_value, alpha=1))\n",
        "\n",
        "            elif classiferName == \"rf\":\n",
        "                classifier = OneVsRestClassifier(\n",
        "                    RandomForestClassifier(n_estimators=100,\n",
        "                                           random_state=random_state_value))\n",
        "\n",
        "            else:\n",
        "                print(\"Classifier not in the list!\")\n",
        "                exit()\n",
        "            Y_score = classifier.fit(X_train, Y_train).predict_proba(X_test)\n",
        "            return Y_score\n",
        "\n",
        "        batch_size = int(self.batch_size)\n",
        "\n",
        "        if (self.dataset_name == \"mnist\"):\n",
        "\n",
        "            n_class = np.zeros(10)\n",
        "            n_class[0] = 5923 - batch_size\n",
        "            n_class[1] = 6742\n",
        "            n_class[2] = 5958\n",
        "            n_class[3] = 6131\n",
        "            n_class[4] = 5842\n",
        "            n_class[5] = 5421\n",
        "            n_class[6] = 5918\n",
        "            n_class[7] = 6265\n",
        "            n_class[8] = 5851\n",
        "            n_class[9] = 5949\n",
        "\n",
        "            Z_sample = np.random.uniform(-1, 1, size=(batch_size, self.z_dim))\n",
        "            y = np.zeros(batch_size, dtype=np.int64) + 0\n",
        "            y_one_hot = np.zeros((batch_size, self.y_dim))\n",
        "            y_one_hot[np.arange(batch_size), y] = 1\n",
        "            images = self.sess.run(self.fake_images,\n",
        "                                   feed_dict={\n",
        "                                       self.z: Z_sample,\n",
        "                                       self.y: y_one_hot\n",
        "                                   })\n",
        "\n",
        "            for classLabel in range(0, 10):\n",
        "                for _ in range(0, int(n_class[classLabel]), batch_size):\n",
        "                    Z_sample = np.random.uniform(-1,\n",
        "                                                 1,\n",
        "                                                 size=(batch_size, self.z_dim))\n",
        "                    y = np.zeros(batch_size, dtype=np.int64) + classLabel\n",
        "                    y_one_hot_init = np.zeros((batch_size, self.y_dim))\n",
        "                    y_one_hot_init[np.arange(batch_size), y] = 1\n",
        "\n",
        "                    images = np.append(images,\n",
        "                                       self.sess.run(self.fake_images,\n",
        "                                                     feed_dict={\n",
        "                                                         self.z: Z_sample,\n",
        "                                                         self.y: y_one_hot_init\n",
        "                                                     }),\n",
        "                                       axis=0)\n",
        "                    y_one_hot = np.append(y_one_hot, y_one_hot_init, axis=0)\n",
        "\n",
        "            X_test, Y_test = load_mnist(train = False)\n",
        "\n",
        "            Y_test = [int(y) for y in Y_test]\n",
        "\n",
        "            classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "            Y_test = label_binarize(Y_test, classes=classes)\n",
        "\n",
        "        if (self.dataset_name == \"cifar10\"):\n",
        "            n_class = np.zeros(10)\n",
        "            for t in range(1, 10):\n",
        "                n_class[t] = 1000\n",
        "            \n",
        "            Z_sample = np.random.uniform(-1, 1, size=(batch_size, self.z_dim))\n",
        "            y = np.zeros(batch_size, dtype=np.int64) + 0\n",
        "            y_one_hot = np.zeros((batch_size, self.y_dim))\n",
        "            y_one_hot[np.arange(batch_size), y] = 1\n",
        "            images = self.sess.run(self.fake_images,\n",
        "                                   feed_dict={\n",
        "                                       self.z: Z_sample,\n",
        "                                       self.y: y_one_hot\n",
        "                                   })\n",
        "\n",
        "            for classLabel in range(0, 10):\n",
        "                for _ in range(0, int(n_class[classLabel]), batch_size):\n",
        "                    Z_sample = np.random.uniform(-1,\n",
        "                                                 1,\n",
        "                                                 size=(batch_size, self.z_dim))\n",
        "                    y = np.zeros(batch_size, dtype=np.int64) + classLabel\n",
        "                    y_one_hot_init = np.zeros((batch_size, self.y_dim))\n",
        "                    y_one_hot_init[np.arange(batch_size), y] = 1\n",
        "\n",
        "                    images = np.append(images,\n",
        "                                       self.sess.run(self.fake_images,\n",
        "                                                     feed_dict={\n",
        "                                                         self.z: Z_sample,\n",
        "                                                         self.y: y_one_hot_init\n",
        "                                                     }),\n",
        "                                       axis=0)\n",
        "                    y_one_hot = np.append(y_one_hot, y_one_hot_init, axis=0)\n",
        "\n",
        "            X_test, Y_test = load_cifar10(train=False)\n",
        "\n",
        "            classes = range(0, 10)\n",
        "            Y_test = label_binarize(Y_test, classes=classes)\n",
        "\n",
        "        print(\"  Classifying - Logistic Regression...\")\n",
        "\n",
        "        TwoDim_images = images.reshape(np.shape(images)[0], -2)\n",
        " \n",
        "        X_test = X_test.reshape(np.shape(X_test)[0], -2)\n",
        "        Y_score = classify(TwoDim_images,\n",
        "                           y_one_hot,\n",
        "                           X_test,\n",
        "                           \"lr\",\n",
        "                           random_state_value=30)\n",
        "\n",
        "        false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(\n",
        "            Y_test, Y_score)\n",
        "\n",
        "        classification_results_fname = self.base_dir + \"CGAN_AuROC.txt\"\n",
        "        classification_results = open(classification_results_fname, \"w\")\n",
        "\n",
        "        classification_results.write(\n",
        "            \"\\nepsilon : {:.2f}, sigma: {:.2f}, clipping value: {:.2f}\".format(\n",
        "                (self.epsilon), round(self.noise_multiplier, 2),\n",
        "                round(self.l2_norm_clip, 2)))\n",
        "\n",
        "        classification_results.write(\"\\nAuROC - logistic Regression: \" +\n",
        "                                     str(roc_auc[\"micro\"]))\n",
        "        classification_results.write(\n",
        "            \"\\n--------------------------------------------------------------------\\n\"\n",
        "        )\n",
        "      \n",
        "        print(\"  Classifying - Random Forest...\")\n",
        "        Y_score = classify(TwoDim_images,\n",
        "                           y_one_hot,\n",
        "                           X_test,\n",
        "                           \"rf\",\n",
        "                           random_state_value=30)\n",
        "\n",
        "        print(\"  Computing ROC - Random Forest ...\")\n",
        "        false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(\n",
        "            Y_test, Y_score)\n",
        "\n",
        "        classification_results.write(\n",
        "            \"\\nepsilon : {:.2f}, sigma: {:.2f}, clipping value: {:.2f}\".format(\n",
        "                (self.epsilon), round(self.noise_multiplier, 2),\n",
        "                round(self.l2_norm_clip, 2)))\n",
        "\n",
        "        classification_results.write(\"\\nAuROC - random Forest: \" +\n",
        "                                     str(roc_auc[\"micro\"]))\n",
        "        classification_results.write(\n",
        "            \"\\n--------------------------------------------------------------------\\n\"\n",
        "        )\n",
        "      \n",
        "        print(\"  Classifying - multilayer Perceptron ...\")\n",
        "        Y_score = classify(TwoDim_images,\n",
        "                           y_one_hot,\n",
        "                           X_test,\n",
        "                           \"mlp\",\n",
        "                           random_state_value=30)\n",
        "\n",
        "        print(\"  Computing ROC - Multilayer Perceptron ...\")\n",
        "        false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(\n",
        "            Y_test, Y_score)\n",
        "\n",
        "        classification_results.write(\n",
        "            \"\\nepsilon : {:.2f}, sigma: {:.2f}, clipping value: {:.2f}\".format(\n",
        "                (self.epsilon), round(self.noise_multiplier, 2),\n",
        "                round(self.l2_norm_clip, 2)))\n",
        "\n",
        "        classification_results.write(\"\\nAuROC - multilayer Perceptron: \" +\n",
        "                                     str(roc_auc[\"micro\"]))\n",
        "        classification_results.write(\n",
        "            \"\\n--------------------------------------------------------------------\\n\"\n",
        "        )\n",
        "\n",
        "        # save model for final step\n",
        "        self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "    def compute_epsilon(self, steps):\n",
        "        \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
        "        if self.noise_multiplier == 0.0:\n",
        "            return float('inf')\n",
        "        orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
        "        sampling_probability = self.batch_size / 60000\n",
        "        rdp = compute_rdp(q=sampling_probability,\n",
        "                          noise_multiplier=self.noise_multiplier,\n",
        "                          steps=steps,\n",
        "                          orders=orders)\n",
        "        # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
        "        return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
        "\n",
        "    # CIFAR 10\n",
        "    def visualize_results_CIFAR(self, epoch):\n",
        "        tot_num_samples = min(self.sample_num, self.batch_size)  # 64, 100\n",
        "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))  # 8\n",
        "        \"\"\" random condition, random noise \"\"\"\n",
        "        y = np.random.choice(self.y_dim, self.batch_size)\n",
        "        y_one_hot = np.zeros((self.batch_size, self.y_dim))\n",
        "        y_one_hot[np.arange(self.batch_size), y] = 1\n",
        "\n",
        "        z_sample = np.random.uniform(-1, 1, size=(self.batch_size,\n",
        "                                                  self.z_dim))  # 100, 100\n",
        "\n",
        "        samples = self.sess.run(self.fake_images,\n",
        "                                feed_dict={\n",
        "                                    self.z: z_sample,\n",
        "                                    self.y: y_one_hot\n",
        "                                })\n",
        "\n",
        "        save_matplot_img(\n",
        "            samples[:image_frame_dim * image_frame_dim, :, :, :],\n",
        "            [image_frame_dim, image_frame_dim], self.result_dir + '/' +\n",
        "            self.model_name + '_epoch%03d' % epoch + '_test_all_classes.png')\n",
        "\n",
        "    # MNIST\n",
        "    def visualize_results_MNIST(self, epoch):\n",
        "        tot_num_samples = min(self.sample_num, self.batch_size)\n",
        "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n",
        "        \"\"\" random condition, random noise \"\"\"\n",
        "        y = np.random.choice(self.y_dim, self.batch_size)\n",
        "        y_one_hot = np.zeros((self.batch_size, self.y_dim))\n",
        "        y_one_hot[np.arange(self.batch_size), y] = 1\n",
        "\n",
        "        z_sample = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
        "        samples = self.sess.run(self.fake_images,\n",
        "                                feed_dict={\n",
        "                                    self.z: z_sample,\n",
        "                                    self.y: y_one_hot\n",
        "                                })\n",
        "\n",
        "        save_images(\n",
        "            samples[:image_frame_dim * image_frame_dim, :, :, :],\n",
        "            [image_frame_dim, image_frame_dim],\n",
        "            check_folder(self.result_dir + '/' + self.model_dir) + '/' +\n",
        "            self.model_name + '_epoch%03d' % epoch + '_test_all_classes.png')\n",
        "        \"\"\" specified condition, random noise \"\"\"\n",
        "        n_styles = 10  # must be less than or equal to self.batch_size\n",
        "\n",
        "        np.random.seed()\n",
        "        si = np.random.choice(self.batch_size, n_styles)\n",
        "\n",
        "        for l in range(self.y_dim):\n",
        "            y = np.zeros(self.batch_size, dtype=np.int64) + l\n",
        "            y_one_hot = np.zeros((self.batch_size, self.y_dim))\n",
        "            y_one_hot[np.arange(self.batch_size), y] = 1\n",
        "\n",
        "            samples = self.sess.run(self.fake_images,\n",
        "                                    feed_dict={\n",
        "                                        self.z: z_sample,\n",
        "                                        self.y: y_one_hot\n",
        "                                    })\n",
        "            save_images(\n",
        "                samples[:image_frame_dim * image_frame_dim, :, :, :],\n",
        "                [image_frame_dim, image_frame_dim],\n",
        "                check_folder(self.result_dir + '/' + self.model_dir) + '/' +\n",
        "                self.model_name + '_epoch%03d' % epoch +\n",
        "                '_test_class_%d.png' % l)\n",
        "\n",
        "            samples = samples[si, :, :, :]\n",
        "\n",
        "            if l == 0:\n",
        "                all_samples = samples\n",
        "            else:\n",
        "                all_samples = np.concatenate((all_samples, samples), axis=0)\n",
        "        \"\"\" save merged images to check style-consistency \"\"\"\n",
        "        canvas = np.zeros_like(all_samples)\n",
        "        for s in range(n_styles):\n",
        "            for c in range(self.y_dim):\n",
        "                canvas[s * self.y_dim +\n",
        "                       c, :, :, :] = all_samples[c * n_styles + s, :, :, :]\n",
        "\n",
        "        save_images(\n",
        "            canvas, [n_styles, self.y_dim],\n",
        "            check_folder(self.result_dir + '/' + self.model_dir) + '/' +\n",
        "            self.model_name + '_epoch%03d' % epoch +\n",
        "            '_test_all_classes_style_by_style.png')\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        return \"{}_{}_{}_{}\".format(self.model_name, self.dataset_name,\n",
        "                                    self.batch_size, self.z_dim)\n",
        "\n",
        "    def save(self, checkpoint_dir, step):\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir,\n",
        "                                      self.model_name)\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        self.saver.save(self.sess,\n",
        "                        os.path.join(checkpoint_dir,\n",
        "                                     self.model_name + '.model'),\n",
        "                        global_step=step)\n",
        "\n",
        "    def load(self, checkpoint_dir):\n",
        "        import re\n",
        "        print(\" [*] Reading checkpoints...\")\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir,\n",
        "                                      self.model_name)\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "            self.saver.restore(self.sess,\n",
        "                               os.path.join(checkpoint_dir, ckpt_name))\n",
        "            counter = int(\n",
        "                next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
        "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
        "            return True, counter\n",
        "        else:\n",
        "            print(\" [*] Failed to find a checkpoint\")\n",
        "            return False, 0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "RPcYboVZSO4G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## gan.utils"
      ]
    },
    {
      "metadata": {
        "id": "-xux9arV_PWL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Most codes from https://github.com/carpedm20/DCGAN-tensorflow\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "\n",
        "from six.moves import xrange\n",
        "import matplotlib.pyplot as plt\n",
        "import os, gzip\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "def one_hot(x, n):\n",
        "    \"\"\"\n",
        "        convert index representation to one-hot representation\n",
        "    \"\"\"\n",
        "    x = np.array(x)\n",
        "    assert x.ndim == 1\n",
        "    return np.eye(n)[x]\n",
        "\n",
        "\n",
        "def prepare_input(data=None, labels=None):\n",
        "    image_height = 32\n",
        "    image_width = 32\n",
        "    image_depth = 3\n",
        "    assert (data.shape[1] == image_height * image_width * image_depth)\n",
        "    assert (data.shape[0] == labels.shape[0])\n",
        "    # do mean normalization across all samples\n",
        "    mu = np.mean(data, axis=0)\n",
        "    mu = mu.reshape(1, -1)\n",
        "    sigma = np.std(data, axis=0)\n",
        "    sigma = sigma.reshape(1, -1)\n",
        "    data = data - mu\n",
        "    data = data / sigma\n",
        "    is_nan = np.isnan(data)\n",
        "    is_inf = np.isinf(data)\n",
        "    if np.any(is_nan) or np.any(is_inf):\n",
        "        print('data is not well-formed : is_nan {n}, is_inf: {i}'.format(\n",
        "            n=np.any(is_nan), i=np.any(is_inf)))\n",
        "    # data is transformed from (no_of_samples, 3072) to (no_of_samples , image_height, image_width, image_depth)\n",
        "    # make sure the type of the data is no.float32\n",
        "    data = data.reshape([-1, image_depth, image_height, image_width])\n",
        "    data = data.transpose([0, 2, 3, 1])\n",
        "    data = data.astype(np.float32)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def read_cifar10(filename):  # queue one element\n",
        "    class CIFAR10Record(object):\n",
        "        pass\n",
        "\n",
        "    result = CIFAR10Record()\n",
        "\n",
        "    label_bytes = 1  # 2 for CIFAR-100\n",
        "    result.height = 32\n",
        "    result.width = 32\n",
        "    result.depth = 3\n",
        "\n",
        "    data = np.load(filename, encoding='latin1')\n",
        "\n",
        "    value = np.asarray(data['data']).astype(np.float32)\n",
        "    labels = np.asarray(data['labels']).astype(np.int32)\n",
        "\n",
        "    return prepare_input(value, labels)\n",
        "\n",
        "\n",
        "def load_cifar10(train):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    if (train == True):\n",
        "\n",
        "        dataX = x_train.reshape([-1, 32, 32, 3])\n",
        "        dataY = y_train\n",
        "\n",
        "    else:\n",
        "        dataX = x_test.reshape([-1, 32, 32, 3])\n",
        "        dataY = y_test\n",
        "\n",
        "    seed = 547\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(dataX)\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(dataY)\n",
        "\n",
        "    y_vec = np.zeros((len(dataY), 10), dtype=np.float)\n",
        "    for i, label in enumerate(dataY):\n",
        "        y_vec[i, dataY[i]] = 1.0\n",
        "\n",
        "    return dataX / 255., y_vec\n",
        "\n",
        "\n",
        "def load_mnist(train = True):\n",
        "\n",
        "    def extract_data(filename, num_data, head_size, data_size):\n",
        "        with gzip.open(filename) as bytestream:\n",
        "            bytestream.read(head_size)\n",
        "            buf = bytestream.read(data_size * num_data)\n",
        "            data = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
        "        return data\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    \n",
        "    x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "    y_train = y_train.reshape((60000))\n",
        "\n",
        "    x_test = x_test.reshape((10000, 28, 28, 1))\n",
        "    y_test = y_test.reshape((10000))\n",
        "\n",
        "    y_train = np.asarray(y_train)\n",
        "    y_test = np.asarray(y_test)\n",
        "\n",
        "    if (train == True):\n",
        "        seed = 547\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(x_train)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(y_train)\n",
        "\n",
        "        y_vec = np.zeros((len(y_train), 10), dtype=np.float)\n",
        "        for i, label in enumerate(y_train):\n",
        "            y_vec[i, y_train[i]] = 1.0\n",
        "        return x_train / 255., y_vec\n",
        "    \n",
        "    else:\n",
        "        seed = 547\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(x_test)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(y_test)\n",
        "\n",
        "        y_vec = np.zeros((len(y_test), 10), dtype=np.float)\n",
        "        for i, label in enumerate(y_test):\n",
        "            y_vec[i, y_test[i]] = 1.0\n",
        "        return x_test / 255., y_vec\n",
        "      \n",
        "\n",
        "\n",
        "def check_folder(log_dir):\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "\n",
        "def show_all_variables():\n",
        "    model_vars = tf.trainable_variables()\n",
        "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "\n",
        "\n",
        "def get_image(image_path,\n",
        "              input_height,\n",
        "              input_width,\n",
        "              resize_height=64,\n",
        "              resize_width=64,\n",
        "              crop=True,\n",
        "              grayscale=False):\n",
        "    image = imread(image_path, grayscale)\n",
        "    return transform(image, input_height, input_width, resize_height,\n",
        "                     resize_width, crop)\n",
        "\n",
        "\n",
        "def save_images(images, size, image_path):\n",
        "    return imsave(inverse_transform(images), size, image_path)\n",
        "\n",
        "\n",
        "def imread(path, grayscale=False):\n",
        "    if (grayscale):\n",
        "        return scipy.misc.imread(path, flatten=True).astype(np.float)\n",
        "    else:\n",
        "        return scipy.misc.imread(path).astype(np.float)\n",
        "\n",
        "\n",
        "def merge_images(images, size):\n",
        "    return inverse_transform(images)\n",
        "\n",
        "\n",
        "def merge(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    if (images.shape[3] in (3, 4)):\n",
        "        c = images.shape[3]\n",
        "        img = np.zeros((h * size[0], w * size[1], c))\n",
        "        for idx, image in enumerate(images):\n",
        "            i = idx % size[1]\n",
        "            j = idx // size[1]\n",
        "            img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "        return img\n",
        "    elif images.shape[3] == 1:\n",
        "        img = np.zeros((h * size[0], w * size[1]))\n",
        "        for idx, image in enumerate(images):\n",
        "            i = idx % size[1]\n",
        "            j = idx // size[1]\n",
        "            img[j * h:j * h + h, i * w:i * w + w] = image[:, :, 0]\n",
        "        return img\n",
        "    else:\n",
        "        raise ValueError('in merge(images,size) images parameter '\n",
        "                         'must have dimensions: HxW or HxWx3 or HxWx4')\n",
        "\n",
        "\n",
        "def imsave(images, size, path):\n",
        "    image = np.squeeze(merge(images, size))\n",
        "    return scipy.misc.imsave(path, image)\n",
        "\n",
        "\n",
        "def center_crop(x, crop_h, crop_w, resize_h=64, resize_w=64):\n",
        "    if crop_w is None:\n",
        "        crop_w = crop_h\n",
        "    h, w = x.shape[:2]\n",
        "    j = int(round((h - crop_h) / 2.))\n",
        "    i = int(round((w - crop_w) / 2.))\n",
        "    return scipy.misc.imresize(x[j:j + crop_h, i:i + crop_w],\n",
        "                               [resize_h, resize_w])\n",
        "\n",
        "\n",
        "def transform(image,\n",
        "              input_height,\n",
        "              input_width,\n",
        "              resize_height=64,\n",
        "              resize_width=64,\n",
        "              crop=True):\n",
        "    if crop:\n",
        "        cropped_image = center_crop(image, input_height, input_width,\n",
        "                                    resize_height, resize_width)\n",
        "    else:\n",
        "        cropped_image = scipy.misc.imresize(image,\n",
        "                                            [resize_height, resize_width])\n",
        "    return np.array(cropped_image) / 127.5 - 1.\n",
        "\n",
        "\n",
        "def inverse_transform(images):\n",
        "    return (images + 1.) / 2.\n",
        "\n",
        "\n",
        "\"\"\" Drawing Tools \"\"\"\n",
        "\n",
        "\n",
        "# borrowed from https://github.com/ykwon0407/variational_autoencoder/blob/master/variational_bayes.ipynb\n",
        "def save_scattered_image(z,\n",
        "                         id,\n",
        "                         z_range_x,\n",
        "                         z_range_y,\n",
        "                         name='scattered_image.jpg'):\n",
        "    N = 10\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(z[:, 0],\n",
        "                z[:, 1],\n",
        "                c=np.argmax(id, 1),\n",
        "                marker='o',\n",
        "                edgecolor='none',\n",
        "                cmap=discrete_cmap(N, 'jet'))\n",
        "    plt.colorbar(ticks=range(N))\n",
        "    axes = plt.gca()\n",
        "    axes.set_xlim([-z_range_x, z_range_x])\n",
        "    axes.set_ylim([-z_range_y, z_range_y])\n",
        "    plt.grid(True)\n",
        "    plt.savefig(name)\n",
        "\n",
        "\n",
        "# borrowed from https://gist.github.com/jakevdp/91077b0cae40f8f8244a\n",
        "def discrete_cmap(N, base_cmap=None):\n",
        "    \"\"\"Create an N-bin discrete colormap from the specified input map\"\"\"\n",
        "\n",
        "    # Note that if base_cmap is a string or None, you can simply do\n",
        "    #    return plt.cm.get_cmap(base_cmap, N)\n",
        "    # The following works for string, None, or a colormap instance:\n",
        "\n",
        "    base = plt.cm.get_cmap(base_cmap)\n",
        "    color_list = base(np.linspace(0, 1, N))\n",
        "    cmap_name = base.name + str(N)\n",
        "    return base.from_list(cmap_name, color_list, N)\n",
        "\n",
        "\n",
        "def save_matplot_img(images, size, image_path):\n",
        "    # revice image data // M*N*3 // RGB float32 : value must set between 0. with 1.\n",
        "    for idx in range(64):\n",
        "        vMin = np.amin(images[idx])\n",
        "        vMax = np.amax(images[idx])\n",
        "        img_arr = images[idx].reshape(32 * 32 * 3, 1)  # flatten\n",
        "        for i, v in enumerate(img_arr):\n",
        "            img_arr[i] = (v - vMin) / (vMax - vMin)\n",
        "        img_arr = img_arr.reshape(32, 32, 3)  # M*N*3\n",
        "\n",
        "        plt.subplot(8, 8, idx + 1), plt.imshow(img_arr,\n",
        "                                               interpolation='nearest')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "    plt.savefig(image_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kh060foYSIxD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Main"
      ]
    },
    {
      "metadata": {
        "id": "BwCDKy5L_DEd",
        "colab_type": "code",
        "outputId": "5351103e-314d-4bbe-e583-137e5d002083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1357
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "base_dir = \"./\"\n",
        "\n",
        "out_dir = base_dir + \"mnist_clip1_sigma0.6_lr0.55\"\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.mkdir(out_dir)\n",
        "\n",
        "gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
        "                                      gpu_options=gpu_options)) as sess:\n",
        "\n",
        "    epoch = 100\n",
        "    cgan = OUR_DP_CGAN(sess,\n",
        "                       epoch=epoch,\n",
        "                       batch_size=64,\n",
        "                       z_dim=100,\n",
        "                       epsilon=9.6,\n",
        "                       delta=1e-5,\n",
        "                       sigma=0.6,\n",
        "                       clip_value=1,\n",
        "                       lr=0.055,\n",
        "                       dataset_name='mnist',\n",
        "                       checkpoint_dir=out_dir + \"/checkpoint/\",\n",
        "                       result_dir=out_dir + \"/results/\",\n",
        "                       log_dir=out_dir + \"/logs/\",\n",
        "                       base_dir=base_dir)\n",
        "\n",
        "    cgan.build_model()\n",
        "    print(\" [*] Building model finished!\")\n",
        "\n",
        "    show_all_variables()\n",
        "    cgan.train()\n",
        "    print(\" [*] Training finished!\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            " [*] Building model finished!\n",
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "discriminator/d_conv1/w:0 (float32_ref 4x4x11x64) [11264, bytes: 45056]\n",
            "discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]\n",
            "discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
            "discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]\n",
            "discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]\n",
            "discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
            "discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
            "discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
            "discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]\n",
            "discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]\n",
            "generator/g_fc1/Matrix:0 (float32_ref 110x1024) [112640, bytes: 450560]\n",
            "generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
            "generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
            "generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
            "generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]\n",
            "generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]\n",
            "generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]\n",
            "generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]\n",
            "generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
            "generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]\n",
            "generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]\n",
            "generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]\n",
            "Total size of variables: 13258754\n",
            "Total bytes of variables: 53035016\n",
            " [*] Reading checkpoints...\n",
            " [*] Failed to find a checkpoint\n",
            " [!] Load failed...\n",
            "Iteration : 98 Eps: 5.750369579644922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:207: DeprecationWarning: `imsave` is deprecated!\n",
            "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imwrite`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration : 198 Eps: 6.088221617339909\n",
            "Iteration : 298 Eps: 6.316992921084355\n",
            "Iteration : 398 Eps: 6.505090299808932\n",
            "Iteration : 498 Eps: 6.6696494264479\n",
            "Iteration : 598 Eps: 6.79218255319497\n",
            "Iteration : 698 Eps: 6.914715679942041\n",
            "Iteration : 798 Eps: 7.037248806689111\n",
            "Iteration : 898 Eps: 7.153695366365903\n",
            "Iteration : 61 Eps: 7.238063480353059\n",
            "Iteration : 161 Eps: 7.322431594340215\n",
            "Iteration : 261 Eps: 7.406799708327371\n",
            "Iteration : 361 Eps: 7.491167822314528\n",
            "Iteration : 461 Eps: 7.5755359363016845\n",
            "Iteration : 561 Eps: 7.659904050288841\n",
            "Iteration : 661 Eps: 7.744272164275998\n",
            "Iteration : 761 Eps: 7.8100417154796835\n",
            "Iteration : 861 Eps: 7.871156710676289\n",
            "Iteration : 24 Eps: 7.932271705872896\n",
            "Iteration : 124 Eps: 7.993386701069502\n",
            "Iteration : 224 Eps: 8.054501696266108\n",
            "Iteration : 324 Eps: 8.115616691462714\n",
            "Iteration : 424 Eps: 8.176731686659322\n",
            "Iteration : 524 Eps: 8.237846681855927\n",
            "Iteration : 624 Eps: 8.298961677052533\n",
            "Iteration : 724 Eps: 8.36007667224914\n",
            "Iteration : 824 Eps: 8.421191667445745\n",
            "Iteration : 924 Eps: 8.482306662642351\n",
            "Iteration : 87 Eps: 8.537226070323127\n",
            "Iteration : 187 Eps: 8.583521710582504\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}